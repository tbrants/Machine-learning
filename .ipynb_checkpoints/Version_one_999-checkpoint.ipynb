{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a01387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import seaborn as sns\n",
    "from proj1_helpers import *\n",
    "\n",
    "SEED = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bfb138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "#its my datapath\n",
    "DATA_TRAIN_PATH = r'C:/Users/Tomas/GitHub/ML_course/projects/project1/data/train1.csv/train.csv' # TODO: download train data and supply path here\n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(y.shape)\n",
    "print(tx.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2dd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manipulate -999 values\n",
    "def manipulate_missing_values(tx):\n",
    "    tx[tx == -999.0] = np.nan\n",
    "    avg_column = np.array([np.nanmean(tx, axis=0)])\n",
    "    avg_column_ = np.repeat(avg_column, tx.shape[0], axis=0)\n",
    "    one_dim_indices_to_subst = np.where(np.isnan(tx))\n",
    "    two_dim_indices_to_subst = np.squeeze(np.dstack((one_dim_indices_to_subst[0],one_dim_indices_to_subst[1])), axis=0)\n",
    "    tx[two_dim_indices_to_subst] = avg_column_[two_dim_indices_to_subst]\n",
    "    return tx\n",
    "tx = manipulate_missing_values(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb32691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9972ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse calculation\n",
    "def compute_error(y, tx, w):\n",
    "    e = y-np.matmul(tx, w)\n",
    "    return e\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = compute_error(y, tx, w)\n",
    "    mse = 1/(2*N)*np.sum(e**2)\n",
    "    rmse = np.sqrt(2*mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583fbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "def compute_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = compute_error(y, tx, w)\n",
    "    grad = (-1/N)*np.matmul(tx.T,e)\n",
    "    return grad\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w -= gamma*grad\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    print(\"Gradient Descent: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da57d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic gradient descent\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = compute_error(y, tx, w)\n",
    "    grad = (-1/N)*np.matmul(tx.T,e)\n",
    "    return grad\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma,  batch_size = 1):\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for ymini, txmini in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            grad = compute_gradient(ymini, txmini, w)\n",
    "            w -= gamma*grad\n",
    "    loss = compute_loss(ymini, txmini, w)\n",
    "    print(\"Stochastic Gradient Descent: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a20d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#least squares\n",
    "def least_squares(y, tx):\n",
    "    N = len(y)\n",
    "    w = np.linalg.solve(np.matmul(tx.T, tx), np.matmul(tx.T,y))\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    print(\"Least squares: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d570b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression\n",
    "def ridge_regression(y, phi, lambda_):\n",
    "    N=len(phi)\n",
    "    lambda_acc=2*N*lambda_\n",
    "    kwad = np.matmul(phi.T,phi)\n",
    "    w = np.linalg.solve(kwad+lambda_acc*np.eye(kwad.shape[0]),np.matmul(phi.T,y))\n",
    "    loss = compute_loss(y, phi, w)\n",
    "    print(\"Ridge regression: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfd20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions for logistic regression\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    return 1/(1 + np.exp(-t))\n",
    "\n",
    "def calculate_loss_lr(y, tx, w):\n",
    "    eps = 1e-6\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(np.matmul(tx, w))\n",
    "    a = np.matmul(y.T, np.log(pred+eps)) \n",
    "    b = np.matmul((1-y).T, np.log(1-pred+eps))\n",
    "    loss = a+b\n",
    "    return np.sum(- loss)\n",
    "\n",
    "def calculate_gradient_lr(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(np.matmul(tx, w)) \n",
    "    grad = np.matmul(tx.T,(pred - y))\n",
    "    return grad\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    loss = calculate_loss_lr(y, tx, w) + lambda_*np.sum(w.T.dot(w))\n",
    "    gradient = calculate_gradient_lr(y,tx,w)+2*lambda_*w\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "886a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression gradient descent\n",
    "def logistic_regression_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        grad = calculate_gradient_lr(y, tx, w)\n",
    "        w -= gamma*grad\n",
    "        #loss = calculate_loss_lr(y, tx, w)\n",
    "        #print(loss)\n",
    "    loss = calculate_loss_lr(y, tx, w)\n",
    "    print(\"Logistic regression: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a719c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularized logistic regression\n",
    "def logistic_regression_reg(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        grad = penalized_logistic_regression(y, tx, w, lambda_)[1]\n",
    "        w -= gamma*grad\n",
    "    loss = penalized_logistic_regression(y, tx, w, lambda_)[0]\n",
    "    print(\"Regularized logistic regression: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b128e",
   "metadata": {},
   "source": [
    "# 2) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb40150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def build_poly_2d(x, degree):\\n    N,M = x.shape\\n    phi = np.zeros((N,M*(degree+1)))\\n    print(phi.shape)\\n    for i in range(M):\\n        col = x[:,i]\\n        exp = build_poly(col, degree)\\n        phi[:, i*(degree+1):(i+1)*(degree)] = exp\\n    return phi\\nx = np.array([[1,2,3],[4,5,6]])\\nprint(build_poly_2d(x, 1))'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#polynomial feature augmentation\n",
    "def build_poly(x, degree):\n",
    "    #prof flammarion said it is possible, but i would wait a bit with doing it\n",
    "    return x\n",
    "\"\"\"def build_poly(x, degree):\n",
    "    if degree == 0:\n",
    "        return x\n",
    "    else:\n",
    "        N = len(x)\n",
    "        phi = np.zeros((N,degree+1))\n",
    "        for i in range(degree+1):\n",
    "            phi[:, i] = x**i\n",
    "        return phi\"\"\"\n",
    "\n",
    "\"\"\"def build_poly_2d(x, degree):\n",
    "    N,M = x.shape\n",
    "    phi = np.zeros((N,M*(degree+1)))\n",
    "    print(phi.shape)\n",
    "    for i in range(M):\n",
    "        col = x[:,i]\n",
    "        exp = build_poly(col, degree)\n",
    "        phi[:, i*(degree+1):(i+1)*(degree)] = exp\n",
    "    return phi\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "print(build_poly_2d(x, 1))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254e77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    TP = len(np.where((y_true == 1) & (y_pred == 1))[0])\n",
    "    FP = len(np.where((y_true == -1) & (y_pred == 1))[0])\n",
    "    TN = len(np.where((y_true == -1) & (y_pred == -1))[0])\n",
    "    FN = len(np.where((y_true == 1) & (y_pred == -1))[0])\n",
    "    print([TP,FP,TN,FN])\n",
    "    if TP+FN>0:\n",
    "        Recall = TP/(TP+FN)\n",
    "    else:\n",
    "        Recall = 0.0001\n",
    "    if TP+FP>0:\n",
    "        Precision = TP/(TP+FP)\n",
    "    else:\n",
    "        Precision = 0.00001\n",
    "    return 2*Recall*Precision/(Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7491bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    TP = len(np.where((y_true == 1) & (y_pred == 1))[0])\n",
    "    return (TP / y_true.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac9acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, opt_method, initial_w, max_iters, gamma, lambda_):\n",
    "    x_train_0, y_train_0 = x[k_indices[:k].ravel()], y[k_indices[:k].ravel()]\n",
    "    x_train_1, y_train_1 = x[k_indices[k+1:].ravel()], y[k_indices[k+1:].ravel()]\n",
    "    if x_train_0.shape[0] == 0:\n",
    "        x_train = x_train_1\n",
    "        y_train = y_train_1\n",
    "    if x_train_1.shape[0] == 0:\n",
    "        x_train = x_train_0\n",
    "        y_train = y_train_0\n",
    "    else:\n",
    "        x_train, y_train = np.concatenate((x_train_0, x_train_1), axis=0), np.concatenate((y_train_0, y_train_1), axis=0)\n",
    "    \n",
    "    x_test, y_test = x[k_indices[k]], y[k_indices[k]]\n",
    "    # get k'th subgroup in test, others in train\n",
    "\n",
    "    \n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    if opt_method == least_squares_GD:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, initial_w, max_iters, gamma)\n",
    "        loss_te = compute_loss(y_test, phi_test, w)\n",
    "        \n",
    "        #print('least_squares_GD')\n",
    "\n",
    "    elif opt_method == least_squares_SGD:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, initial_w, max_iters, gamma,  batch_size = 1)\n",
    "        loss_te = compute_loss(y_test, phi_test, w)\n",
    "        #print('least_squares_SGD')\n",
    "    \n",
    "    elif opt_method == least_squares:\n",
    "        w, loss_tr = opt_method(y_train, phi_train)\n",
    "        loss_te = compute_loss(y_test, phi_test, w)\n",
    "        #print('least_squares')\n",
    "\n",
    "    elif opt_method == ridge_regression:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, lambda_)\n",
    "        loss_te = compute_loss(y_test, phi_test, w)\n",
    "        #print('ridge_regression')\n",
    "\n",
    "    elif opt_method == logistic_regression_gd:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, initial_w, max_iters, gamma)\n",
    "        loss_te = calculate_loss_lr(y_test, phi_test, w)\n",
    "        #print('logistic_regression_gd')\n",
    "\n",
    "    elif opt_method == logistic_regression_reg:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, initial_w, max_iters, gamma, lambda_)\n",
    "        loss_te = penalized_logistic_regression(y_test, phi_test, w, lambda_)[0]\n",
    "        #print('reg_logistic_regression')\n",
    "\n",
    "    else:\n",
    "        w, loss = (None , None)\n",
    "        test_loss = None\n",
    "        #print(\"Method not found\")\n",
    "    \n",
    "    \n",
    "    y_pred_tr = predict_labels(w, phi_train)\n",
    "    y_pred_te = predict_labels(w, phi_test)\n",
    "    acc_train = accuracy(y_train, y_pred_tr)\n",
    "    f1_train = f1(y_train, y_pred_tr)\n",
    "    acc_test = accuracy(y_test, y_pred_te)\n",
    "    f1_test = f1(y_test, y_pred_te)\n",
    "    \n",
    "    loss = [loss_tr, loss_te]\n",
    "    acc = [acc_train, acc_test]\n",
    "    f1_val = [f1_train, f1_test]\n",
    "    \n",
    "    return loss, acc, f1_val, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5915f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cross_validation(y, x, k_indices, k_fold, degree, opt_method, initial_w = 0, max_iters = 0, gamma = 0, lambda_ = 0):\n",
    "    loss_tr = []\n",
    "    loss_te = []\n",
    "    acc_tr = []\n",
    "    acc_te = []\n",
    "    f1_tr = []\n",
    "    f1_te = []\n",
    "    for k in range(k_fold):\n",
    "        loss, acc, f1_val, w = cross_validation(y, x, k_indices, k, degree, opt_method, initial_w, max_iters, gamma, lambda_)\n",
    "        loss_tr.append(loss[0])\n",
    "        loss_te.append(loss[1])\n",
    "        acc_tr.append(acc[0])\n",
    "        acc_te.append(acc[1])\n",
    "        f1_tr.append(f1_val[0])\n",
    "        f1_te.append(f1_val[1])\n",
    "    rmse_tr = np.mean(np.array(loss_tr))\n",
    "    rmse_te = np.mean(np.array(loss_te))\n",
    "    acc_tr_ = np.mean(np.array(acc_tr))\n",
    "    acc_te_ = np.mean(np.array(acc_te))\n",
    "    f1_tr_ = np.mean(np.array(f1_tr))\n",
    "    f1_te_ = np.mean(np.array(f1_te))\n",
    "    print(\"Averaged train rmse after %d-fold cross-validation = %.8f\"%(k_fold, rmse_tr))\n",
    "    print(\"Averaged test rmse after %d-fold cross-validation = %.8f\"%(k_fold, rmse_te))\n",
    "    print(\"Averaged train accuracy after %d-fold cross-validation = %.2f\"%(k_fold, acc_tr_))\n",
    "    print(\"Averaged test accuracy after %d-fold cross-validation = %.2f\"%(k_fold, acc_te_))\n",
    "    print(\"Averaged train F1-score after %d-fold cross-validation = %.2f\"%(k_fold, f1_tr_))\n",
    "    print(\"Averaged test F1-score after %d-fold cross-validation = %.2f\"%(k_fold, f1_te_))\n",
    "    return rmse_tr, rmse_te, acc_tr_, acc_te_, f1_tr_, f1_te_, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29568aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4974.979\n",
      "-18.066\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tx.shape[1]))\n",
    "max_iters = 100\n",
    "gamma = 1e-11\n",
    "lambda_ = 0.4\n",
    "print(tx.max())\n",
    "print(tx.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ac7848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: train loss is  0.9999803816442033\n",
      "[0, 0, 123166, 64334]\n",
      "[0, 0, 41167, 21333]\n",
      "Gradient Descent: train loss is  0.9999603484175836\n",
      "[0, 0, 123347, 64153]\n",
      "[0, 0, 40986, 21514]\n",
      "Gradient Descent: train loss is  0.9999407748578134\n",
      "[0, 0, 123258, 64242]\n",
      "[0, 0, 41075, 21425]\n",
      "Gradient Descent: train loss is  0.9999211927547722\n",
      "[0, 0, 123228, 64272]\n",
      "[0, 0, 41105, 21395]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.99995067\n",
      "Averaged test rmse after 4-fold cross-validation = 0.99995070\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.00\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.00\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.00\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.00\n",
      "Stochastic Gradient Descent: train loss is  1.0003519177464768\n",
      "[0, 0, 123166, 64334]\n",
      "[0, 0, 41167, 21333]\n",
      "Stochastic Gradient Descent: train loss is  0.999569293799594\n",
      "[0, 0, 123347, 64153]\n",
      "[0, 0, 40986, 21514]\n",
      "Stochastic Gradient Descent: train loss is  1.0005402641250916\n",
      "[0, 0, 123258, 64242]\n",
      "[0, 0, 41075, 21425]\n",
      "Stochastic Gradient Descent: train loss is  0.9995083382957489\n",
      "[0, 0, 123228, 64272]\n",
      "[0, 0, 41105, 21395]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.99999245\n",
      "Averaged test rmse after 4-fold cross-validation = 0.99986616\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.00\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.00\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.00\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.00\n",
      "Least squares: train loss is  0.9014283825798134\n",
      "[15589, 6277, 116889, 48745]\n",
      "[5191, 2063, 39104, 16142]\n",
      "Least squares: train loss is  0.90129940598269\n",
      "[15368, 6131, 117216, 48785]\n",
      "[5249, 2073, 38913, 16265]\n",
      "Least squares: train loss is  0.9007704017951291\n",
      "[15559, 6189, 117069, 48683]\n",
      "[5120, 2087, 38988, 16305]\n",
      "Least squares: train loss is  0.9012806209781316\n",
      "[15584, 6199, 117029, 48688]\n",
      "[5122, 2077, 39028, 16273]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90119470\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90133415\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.36\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.36\n",
      "Ridge regression: train loss is  0.9014283825798134\n",
      "[15589, 6277, 116889, 48745]\n",
      "[5191, 2063, 39104, 16142]\n",
      "Ridge regression: train loss is  0.90129940598269\n",
      "[15368, 6131, 117216, 48785]\n",
      "[5249, 2073, 38913, 16265]\n",
      "Ridge regression: train loss is  0.9007704017951291\n",
      "[15559, 6189, 117069, 48683]\n",
      "[5120, 2087, 38988, 16305]\n",
      "Ridge regression: train loss is  0.9012806209781316\n",
      "[15584, 6199, 117029, 48688]\n",
      "[5122, 2077, 39028, 16273]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90119470\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90133415\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.36\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.36\n",
      "Logistic regression: train loss is  -773161.082528569\n",
      "[0, 0, 123166, 64334]\n",
      "[0, 0, 41167, 21333]\n",
      "Logistic regression: train loss is  -817361.0825450222\n",
      "[0, 0, 123347, 64153]\n",
      "[0, 0, 40986, 21514]\n",
      "Logistic regression: train loss is  -815333.8804932975\n",
      "[0, 0, 123258, 64242]\n",
      "[0, 0, 41075, 21425]\n",
      "Logistic regression: train loss is  -814507.4695184289\n",
      "[0, 0, 123228, 64272]\n",
      "[0, 0, 41105, 21395]\n",
      "Averaged train rmse after 4-fold cross-validation = -805090.87877133\n",
      "Averaged test rmse after 4-fold cross-validation = -268260.03150802\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.00\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.00\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.00\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.00\n",
      "Regularized logistic regression: train loss is  -812794.3560892523\n",
      "[0, 0, 123166, 64334]\n",
      "[0, 0, 41167, 21333]\n",
      "Regularized logistic regression: train loss is  -817795.5683031484\n",
      "[0, 0, 123347, 64153]\n",
      "[0, 0, 40986, 21514]\n",
      "Regularized logistic regression: train loss is  -815336.4035766041\n",
      "[0, 0, 123258, 64242]\n",
      "[0, 0, 41075, 21425]\n",
      "Regularized logistic regression: train loss is  -814507.468662957\n",
      "[0, 0, 123228, 64272]\n",
      "[0, 0, 41105, 21395]\n",
      "Averaged train rmse after 4-fold cross-validation = -815108.44915799\n",
      "Averaged test rmse after 4-fold cross-validation = -271702.80808422\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.00\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.00\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.00\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-53f4dcbe4310>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "#apply cross validation\n",
    "k_fold = 4\n",
    "degree = 1\n",
    "k_indices = build_k_indices(y, k_fold, SEED)\n",
    "\n",
    "methods = ['LS_GD', 'LS_SGD', 'LS', 'RIDGE', 'LR_GD', 'LR_SGD']\n",
    "\n",
    "results = np.zeros(4*6)\n",
    "results = results.reshape(6,4)\n",
    "\n",
    "rmse_tr_ls_gd, rmse_te_ls_gd, results[0,0], results[0,1], results[0,2], results[0,3], w_gd = apply_cross_validation(y, tx, k_indices, k_fold, degree, least_squares_GD, initial_w, max_iters, gamma, lambda_)\n",
    "\n",
    "rmse_tr_ls_sgd, rmse_te_ls_sgd, results[1,0], results[1,1], results[1,2], results[1,3], w_sgd = apply_cross_validation(y, tx, k_indices, k_fold, degree, least_squares_SGD, initial_w, max_iters, gamma, lambda_)\n",
    "#print(rmse_tr_ls_sgd, rmse_te_ls_sgd)\n",
    "\n",
    "rmse_tr_ls, rmse_te_ls, results[2,0], results[2,1], results[2,2], results[2,3], w_ls = apply_cross_validation(y, tx, k_indices, k_fold, degree, least_squares)\n",
    "#print(rmse_tr_ls, rmse_te_ls)\n",
    "\n",
    "rmse_tr_ridge, rmse_te_ridge, results[3,0], results[3,1], results[3,2], results[3,3], w_rr = apply_cross_validation(y, tx, k_indices, k_fold, degree, ridge_regression, lambda_)\n",
    "#print(rmse_tr_ridge, rmse_te_ridge)\n",
    "\n",
    "rmse_tr_lr_gd, rmse_te_lr_gd, results[4,0], results[4,1], results[4,2], results[4,3], w_lr= apply_cross_validation(y, tx, k_indices, k_fold, degree, logistic_regression_gd, initial_w, max_iters, gamma)\n",
    "#print(rmse_tr_lr_gd, rmse_te_lr_gd)\n",
    "\n",
    "rmse_tr_lr_reg, rmse_te_lr_reg, results[5,0], results[5,1], results[5,2], results[5,3], w_lr_reg = apply_cross_validation(y, tx, k_indices, k_fold, degree, logistic_regression_reg, initial_w, max_iters, gamma, lambda_)\n",
    "#print(rmse_tr_lr_reg, rmse_te_lr_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec8c563f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,3,figsize=(25,14), sharey=True)\n",
    "for res in np.arange(results.shape[0]):\n",
    "        #print(res//3, res%3)\n",
    "        barWidth = 0.3\n",
    "        bars = ('train accuracy', 'test accuracy', 'train f1', 'test f1')\n",
    "\n",
    "        # Create bars\n",
    "        height = results[res]\n",
    "        axs[res//3,res%3].bar(bars, height, color=(0.2, 0.4, 0.6, 0.8))\n",
    "        # Create names on the x-axis\n",
    "        axs[res//3,res%3].set_title(methods[res])\n",
    "        \n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1ccd31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam(method, initial_w = ['DEFAULT'], gamma =['DEFAULT'], lambda_=['DEFAULT'], degree=['DEFAULT']):\n",
    "    max_iters = 100\n",
    "    \n",
    "    initial_w_arr = initial_w\n",
    "    gammas = gamma\n",
    "    degrees = degree\n",
    "    lambdas = lambda_\n",
    "    \n",
    "    max_acc = 0\n",
    "    max_w = 0\n",
    "    \n",
    "    opt_ini = 0\n",
    "    opt_gamma = 0\n",
    "    opt_lambda = 0\n",
    "    opt_degree = 0\n",
    "    for lambda_ in lambda_:\n",
    "        print(lambda_)\n",
    "        for initial_w in initial_w_arr:\n",
    "            for gamma in gammas:\n",
    "               for degree in degrees:\n",
    "                    rmse_tr_ridge, rmse_te_ridge, acc_train, acc_test, f1_train, f1_test, w = apply_cross_validation(y, tx, k_indices, k_fold, degree, method, initial_w, max_iters, gamma, lambda_)\n",
    "                    if acc_test > max_acc:\n",
    "                        max_acc = acc_test\n",
    "                        max_w = w\n",
    "                        opt_ini = initial_w\n",
    "                        opt_gamma = gamma\n",
    "                        opt_lambda = lambda_\n",
    "                        opt_degree = degree\n",
    "    return(max_acc, max_w, opt_ini, opt_gamma, opt_lambda, opt_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c36ab360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Ridge regression: train loss is  0.9056071357744623\n",
      "[14727, 6191, 116975, 49607]\n",
      "[4963, 2012, 39155, 16370]\n",
      "Ridge regression: train loss is  0.9053559029339339\n",
      "[14555, 5988, 117359, 49598]\n",
      "[5013, 2037, 38949, 16501]\n",
      "Ridge regression: train loss is  0.9047980091380434\n",
      "[14801, 6046, 117212, 49441]\n",
      "[4799, 2032, 39043, 16626]\n",
      "Ridge regression: train loss is  0.9052947197274471\n",
      "[14769, 6077, 117151, 49503]\n",
      "[4835, 2019, 39086, 16560]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90526394\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90536078\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.35\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.35\n",
      "1.1090909090909091\n",
      "Ridge regression: train loss is  0.9077676997863469\n",
      "[14232, 5917, 117249, 50102]\n",
      "[4780, 1931, 39236, 16553]\n",
      "Ridge regression: train loss is  0.9074744610008807\n",
      "[14075, 5795, 117552, 50078]\n",
      "[4831, 1950, 39036, 16683]\n",
      "Ridge regression: train loss is  0.9068858118391712\n",
      "[14283, 5775, 117483, 49959]\n",
      "[4630, 1978, 39097, 16795]\n",
      "Ridge regression: train loss is  0.9074710790252951\n",
      "[14242, 5842, 117386, 50030]\n",
      "[4681, 1922, 39183, 16714]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90739976\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90746550\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.34\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.34\n",
      "2.118181818181818\n",
      "Ridge regression: train loss is  0.9080024311313168\n",
      "[14143, 5852, 117314, 50191]\n",
      "[4758, 1919, 39248, 16575]\n",
      "Ridge regression: train loss is  0.9077056178775749\n",
      "[13971, 5721, 117626, 50182]\n",
      "[4796, 1927, 39059, 16718]\n",
      "Ridge regression: train loss is  0.9071197172624961\n",
      "[14184, 5709, 117549, 50058]\n",
      "[4595, 1957, 39118, 16830]\n",
      "Ridge regression: train loss is  0.9076997340885158\n",
      "[14142, 5788, 117440, 50130]\n",
      "[4654, 1898, 39207, 16741]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90763188\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90769139\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.08\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.34\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.34\n",
      "3.1272727272727274\n",
      "Ridge regression: train loss is  0.9081067092435283\n",
      "[14050, 5805, 117361, 50284]\n",
      "[4730, 1910, 39257, 16603]\n",
      "Ridge regression: train loss is  0.9078090601821452\n",
      "[13901, 5679, 117668, 50252]\n",
      "[4769, 1916, 39070, 16745]\n",
      "Ridge regression: train loss is  0.9072255136815391\n",
      "[14110, 5676, 117582, 50132]\n",
      "[4567, 1939, 39136, 16858]\n",
      "Ridge regression: train loss is  0.9078001820178461\n",
      "[14075, 5744, 117484, 50197]\n",
      "[4631, 1886, 39219, 16764]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90773537\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90779175\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "4.136363636363636\n",
      "Ridge regression: train loss is  0.9081713688337256\n",
      "[13991, 5757, 117409, 50343]\n",
      "[4709, 1892, 39275, 16624]\n",
      "Ridge regression: train loss is  0.9078737564186375\n",
      "[13848, 5638, 117709, 50305]\n",
      "[4749, 1897, 39089, 16765]\n",
      "Ridge regression: train loss is  0.9072921150112174\n",
      "[14047, 5629, 117629, 50195]\n",
      "[4555, 1922, 39153, 16870]\n",
      "Ridge regression: train loss is  0.9078626021166545\n",
      "[14003, 5701, 117527, 50269]\n",
      "[4607, 1874, 39231, 16788]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90779996\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90785431\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "5.145454545454545\n",
      "Ridge regression: train loss is  0.908219280682199\n",
      "[13944, 5716, 117450, 50390]\n",
      "[4692, 1879, 39288, 16641]\n",
      "Ridge regression: train loss is  0.9079220904581649\n",
      "[13807, 5595, 117752, 50346]\n",
      "[4732, 1879, 39107, 16782]\n",
      "Ridge regression: train loss is  0.9073420999007202\n",
      "[13977, 5587, 117671, 50265]\n",
      "[4545, 1911, 39164, 16880]\n",
      "Ridge regression: train loss is  0.9079092277551156\n",
      "[13949, 5662, 117566, 50323]\n",
      "[4590, 1857, 39248, 16805]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90784817\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90790103\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "6.154545454545454\n",
      "Ridge regression: train loss is  0.9082589288156792\n",
      "[13904, 5679, 117487, 50430]\n",
      "[4683, 1858, 39309, 16650]\n",
      "Ridge regression: train loss is  0.9079623526115188\n",
      "[13752, 5557, 117790, 50401]\n",
      "[4716, 1859, 39127, 16798]\n",
      "Ridge regression: train loss is  0.9073838710618388\n",
      "[13920, 5541, 117717, 50322]\n",
      "[4533, 1890, 39185, 16892]\n",
      "Ridge regression: train loss is  0.9079481865694223\n",
      "[13901, 5620, 117608, 50371]\n",
      "[4579, 1841, 39264, 16816]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90788833\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90794001\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "7.163636363636363\n",
      "Ridge regression: train loss is  0.9082941469129244\n",
      "[13852, 5656, 117510, 50482]\n",
      "[4668, 1846, 39321, 16665]\n",
      "Ridge regression: train loss is  0.9079982812943355\n",
      "[13693, 5520, 117827, 50460]\n",
      "[4697, 1852, 39134, 16817]\n",
      "Ridge regression: train loss is  0.9074212271046664\n",
      "[13884, 5497, 117761, 50358]\n",
      "[4521, 1880, 39195, 16904]\n",
      "Ridge regression: train loss is  0.9079831079439219\n",
      "[13861, 5581, 117647, 50411]\n",
      "[4563, 1832, 39273, 16832]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90792419\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90797489\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "8.172727272727272\n",
      "Ridge regression: train loss is  0.9083268905988361\n",
      "[13797, 5629, 117537, 50537]\n",
      "[4652, 1829, 39338, 16681]\n",
      "Ridge regression: train loss is  0.9080317794204679\n",
      "[13647, 5488, 117859, 50506]\n",
      "[4678, 1836, 39150, 16836]\n",
      "Ridge regression: train loss is  0.9074561037052753\n",
      "[13842, 5463, 117795, 50400]\n",
      "[4502, 1872, 39203, 16923]\n",
      "Ridge regression: train loss is  0.9080158235471695\n",
      "[13825, 5550, 117678, 50447]\n",
      "[4547, 1829, 39276, 16848]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90795765\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90800752\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "9.181818181818182\n",
      "Ridge regression: train loss is  0.9083582421656136\n",
      "[13761, 5590, 117576, 50573]\n",
      "[4636, 1814, 39353, 16697]\n",
      "Ridge regression: train loss is  0.9080638981107907\n",
      "[13609, 5451, 117896, 50544]\n",
      "[4655, 1824, 39162, 16859]\n",
      "Ridge regression: train loss is  0.9074895702069422\n",
      "[13799, 5438, 117820, 50443]\n",
      "[4491, 1853, 39222, 16934]\n",
      "Ridge regression: train loss is  0.9080473364887602\n",
      "[13777, 5511, 117717, 50495]\n",
      "[4533, 1822, 39283, 16862]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90798976\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90803890\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "10.19090909090909\n",
      "Ridge regression: train loss is  0.9083888359780712\n",
      "[13728, 5553, 117613, 50606]\n",
      "[4623, 1806, 39361, 16710]\n",
      "Ridge regression: train loss is  0.9080952526501169\n",
      "[13563, 5421, 117926, 50590]\n",
      "[4639, 1817, 39169, 16875]\n",
      "Ridge regression: train loss is  0.9075222521676256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13760, 5412, 117846, 50482]\n",
      "[4483, 1847, 39228, 16942]\n",
      "Ridge regression: train loss is  0.9080782283709141\n",
      "[13734, 5478, 117750, 50538]\n",
      "[4518, 1813, 39292, 16877]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90802114\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90806963\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "11.2\n",
      "Ridge regression: train loss is  0.9084190595026745\n",
      "[13683, 5518, 117648, 50651]\n",
      "[4600, 1795, 39372, 16733]\n",
      "Ridge regression: train loss is  0.9081262186973901\n",
      "[13525, 5380, 117967, 50628]\n",
      "[4623, 1808, 39178, 16891]\n",
      "Ridge regression: train loss is  0.9075545310450578\n",
      "[13717, 5377, 117881, 50525]\n",
      "[4464, 1835, 39240, 16961]\n",
      "Ridge regression: train loss is  0.908108850286739\n",
      "[13691, 5438, 117790, 50581]\n",
      "[4512, 1799, 39306, 16883]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90805216\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90810007\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "12.209090909090909\n",
      "Ridge regression: train loss is  0.9084491564221493\n",
      "[13637, 5492, 117674, 50697]\n",
      "[4571, 1784, 39383, 16762]\n",
      "Ridge regression: train loss is  0.9081570327096391\n",
      "[13467, 5357, 117990, 50686]\n",
      "[4613, 1793, 39193, 16901]\n",
      "Ridge regression: train loss is  0.90758664652003\n",
      "[13668, 5349, 117909, 50574]\n",
      "[4450, 1824, 39251, 16975]\n",
      "Ridge regression: train loss is  0.9081394201483848\n",
      "[13646, 5404, 117824, 50626]\n",
      "[4488, 1786, 39319, 16907]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90808306\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90813043\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "13.218181818181817\n",
      "Ridge regression: train loss is  0.9084792829979554\n",
      "[13588, 5461, 117705, 50746]\n",
      "[4555, 1776, 39391, 16778]\n",
      "Ridge regression: train loss is  0.9081878466997593\n",
      "[13422, 5320, 118027, 50731]\n",
      "[4600, 1782, 39204, 16914]\n",
      "Ridge regression: train loss is  0.907618752326266\n",
      "[13619, 5314, 117944, 50623]\n",
      "[4435, 1810, 39265, 16990]\n",
      "Ridge regression: train loss is  0.908170075549961\n",
      "[13595, 5376, 117852, 50677]\n",
      "[4480, 1778, 39327, 16915]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90811399\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90816087\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "14.227272727272727\n",
      "Ridge regression: train loss is  0.9085095404797248\n",
      "[13560, 5421, 117745, 50774]\n",
      "[4539, 1761, 39406, 16794]\n",
      "Ridge regression: train loss is  0.9082187596414963\n",
      "[13370, 5280, 118067, 50783]\n",
      "[4589, 1776, 39210, 16925]\n",
      "Ridge regression: train loss is  0.9076509482767633\n",
      "[13560, 5284, 117974, 50682]\n",
      "[4417, 1803, 39272, 17008]\n",
      "Ridge regression: train loss is  0.9082009039730302\n",
      "[13540, 5343, 117885, 50732]\n",
      "[4463, 1767, 39338, 16932]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90814504\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90819146\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.33\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.33\n",
      "15.236363636363635\n",
      "Ridge regression: train loss is  0.9085399945266226\n",
      "[13529, 5385, 117781, 50805]\n",
      "[4526, 1752, 39415, 16807]\n",
      "Ridge regression: train loss is  0.9082498362391683\n",
      "[13322, 5256, 118091, 50831]\n",
      "[4574, 1773, 39213, 16940]\n",
      "Ridge regression: train loss is  0.9076832994010942\n",
      "[13520, 5251, 118007, 50722]\n",
      "[4405, 1791, 39284, 17020]\n",
      "Ridge regression: train loss is  0.9082319607711493\n",
      "[13486, 5310, 117918, 50786]\n",
      "[4453, 1764, 39341, 16942]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90817627\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90822226\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "16.245454545454546\n",
      "Ridge regression: train loss is  0.9085706872502559\n",
      "[13492, 5363, 117803, 50842]\n",
      "[4513, 1746, 39421, 16820]\n",
      "Ridge regression: train loss is  0.9082811185364553\n",
      "[13284, 5229, 118118, 50869]\n",
      "[4559, 1766, 39220, 16955]\n",
      "Ridge regression: train loss is  0.9077158477748497\n",
      "[13476, 5221, 118037, 50766]\n",
      "[4393, 1782, 39293, 17032]\n",
      "Ridge regression: train loss is  0.9082632802479851\n",
      "[13448, 5275, 117953, 50824]\n",
      "[4438, 1757, 39348, 16957]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90820773\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90825332\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "17.254545454545454\n",
      "Ridge regression: train loss is  0.9086016448995117\n",
      "[13456, 5333, 117833, 50878]\n",
      "[4499, 1736, 39431, 16834]\n",
      "Ridge regression: train loss is  0.9083126333062896\n",
      "[13247, 5210, 118137, 50906]\n",
      "[4543, 1750, 39236, 16971]\n",
      "Ridge regression: train loss is  0.9077486200431321\n",
      "[13433, 5193, 118065, 50809]\n",
      "[4381, 1765, 39310, 17044]\n",
      "Ridge regression: train loss is  0.908294882677687\n",
      "[13403, 5241, 117987, 50869]\n",
      "[4419, 1751, 39354, 16976]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90823945\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90828466\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "18.263636363636365\n",
      "Ridge regression: train loss is  0.9086328828849799\n",
      "[13407, 5303, 117863, 50927]\n",
      "[4477, 1731, 39436, 16856]\n",
      "Ridge regression: train loss is  0.9083443968720272\n",
      "[13211, 5175, 118172, 50942]\n",
      "[4531, 1739, 39247, 16983]\n",
      "Ridge regression: train loss is  0.9077816323227833\n",
      "[13400, 5162, 118096, 50842]\n",
      "[4369, 1757, 39318, 17056]\n",
      "Ridge regression: train loss is  0.9083267788615772\n",
      "[13366, 5212, 118016, 50906]\n",
      "[4404, 1740, 39365, 16991]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90827142\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90831628\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "19.272727272727273\n",
      "Ridge regression: train loss is  0.9086644091327576\n",
      "[13370, 5277, 117889, 50964]\n",
      "[4465, 1724, 39443, 16868]\n",
      "Ridge regression: train loss is  0.908376418319811\n",
      "[13165, 5147, 118200, 50988]\n",
      "[4523, 1734, 39252, 16991]\n",
      "Ridge regression: train loss is  0.9078148934636321\n",
      "[13370, 5132, 118126, 50872]\n",
      "[4358, 1751, 39324, 17067]\n",
      "Ridge regression: train loss is  0.9083589731464295\n",
      "[13331, 5187, 118041, 50941]\n",
      "[4385, 1725, 39380, 17010]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90830367\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90834819\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "20.28181818181818\n",
      "Ridge regression: train loss is  0.9086962263632097\n",
      "[13327, 5244, 117922, 51007]\n",
      "[4448, 1719, 39448, 16885]\n",
      "Ridge regression: train loss is  0.9084087016783393\n",
      "[13127, 5117, 118230, 51026]\n",
      "[4510, 1726, 39260, 17004]\n",
      "Ridge regression: train loss is  0.9078484072572405\n",
      "[13324, 5110, 118148, 50918]\n",
      "[4343, 1738, 39337, 17082]\n",
      "Ridge regression: train loss is  0.9083914654585511\n",
      "[13296, 5156, 118072, 50976]\n",
      "[4377, 1712, 39393, 17018]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90833620\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90838039\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "21.290909090909093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: train loss is  0.908728333663043\n",
      "[13295, 5222, 117944, 51039]\n",
      "[4429, 1714, 39453, 16904]\n",
      "Ridge regression: train loss is  0.9084412474215123\n",
      "[13080, 5088, 118259, 51073]\n",
      "[4497, 1713, 39273, 17017]\n",
      "Ridge regression: train loss is  0.9078821739561465\n",
      "[13287, 5084, 118174, 50955]\n",
      "[4337, 1730, 39345, 17088]\n",
      "Ridge regression: train loss is  0.9084242526948462\n",
      "[13258, 5127, 118101, 51014]\n",
      "[4363, 1704, 39401, 17032]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90836900\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90841288\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "22.3\n",
      "Ridge regression: train loss is  0.9087607275840551\n",
      "[13254, 5182, 117984, 51080]\n",
      "[4419, 1700, 39467, 16914]\n",
      "Ridge regression: train loss is  0.9084740535185969\n",
      "[13044, 5065, 118282, 51109]\n",
      "[4487, 1704, 39282, 17027]\n",
      "Ridge regression: train loss is  0.9079161913329046\n",
      "[13246, 5054, 118204, 50996]\n",
      "[4327, 1723, 39352, 17098]\n",
      "Ridge regression: train loss is  0.9084573296859646\n",
      "[13225, 5097, 118131, 51047]\n",
      "[4354, 1694, 39411, 17041]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90840208\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90844566\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "23.30909090909091\n",
      "Ridge regression: train loss is  0.9087934029195961\n",
      "[13212, 5162, 118004, 51122]\n",
      "[4402, 1691, 39476, 16931]\n",
      "Ridge regression: train loss is  0.9085071161769149\n",
      "[13006, 5039, 118308, 51147]\n",
      "[4474, 1697, 39289, 17040]\n",
      "Ridge regression: train loss is  0.9079504554268718\n",
      "[13199, 5034, 118224, 51043]\n",
      "[4316, 1719, 39356, 17109]\n",
      "Ridge regression: train loss is  0.908490689870047\n",
      "[13183, 5075, 118153, 51089]\n",
      "[4344, 1685, 39420, 17051]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90843542\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90847872\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "24.31818181818182\n",
      "Ridge regression: train loss is  0.9088263532583465\n",
      "[13176, 5134, 118032, 51158]\n",
      "[4390, 1683, 39484, 16943]\n",
      "Ridge regression: train loss is  0.9085404303724508\n",
      "[12968, 5014, 118333, 51185]\n",
      "[4460, 1688, 39298, 17054]\n",
      "Ridge regression: train loss is  0.9079849610759826\n",
      "[13172, 5006, 118252, 51070]\n",
      "[4306, 1705, 39370, 17119]\n",
      "Ridge regression: train loss is  0.9085243257679362\n",
      "[13137, 5055, 118173, 51135]\n",
      "[4332, 1673, 39432, 17063]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90846902\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90851204\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "25.327272727272728\n",
      "Ridge regression: train loss is  0.9088595713822375\n",
      "[13140, 5116, 118050, 51194]\n",
      "[4376, 1674, 39493, 16957]\n",
      "Ridge regression: train loss is  0.9085739902322225\n",
      "[12938, 4983, 118364, 51215]\n",
      "[4447, 1684, 39302, 17067]\n",
      "Ridge regression: train loss is  0.9080197022985372\n",
      "[13125, 4978, 118280, 51117]\n",
      "[4293, 1692, 39383, 17132]\n",
      "Ridge regression: train loss is  0.9085582293204817\n",
      "[13101, 5026, 118202, 51171]\n",
      "[4317, 1663, 39442, 17078]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90850287\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90854563\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "26.336363636363636\n",
      "Ridge regression: train loss is  0.9088930495540137\n",
      "[13111, 5088, 118078, 51223]\n",
      "[4364, 1666, 39501, 16969]\n",
      "Ridge regression: train loss is  0.9086077893117832\n",
      "[12888, 4959, 118388, 51265]\n",
      "[4430, 1675, 39311, 17084]\n",
      "Ridge regression: train loss is  0.9080546725691223\n",
      "[13091, 4954, 118304, 51151]\n",
      "[4279, 1680, 39395, 17146]\n",
      "Ridge regression: train loss is  0.9085923921289791\n",
      "[13077, 5002, 118226, 51195]\n",
      "[4306, 1655, 39450, 17089]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90853698\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90857948\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "27.345454545454544\n",
      "Ridge regression: train loss is  0.9089267797258658\n",
      "[13077, 5064, 118102, 51257]\n",
      "[4355, 1660, 39507, 16978]\n",
      "Ridge regression: train loss is  0.9086418207977446\n",
      "[12855, 4926, 118421, 51298]\n",
      "[4417, 1667, 39319, 17097]\n",
      "Ridge regression: train loss is  0.9080898650190298\n",
      "[13053, 4936, 118322, 51189]\n",
      "[4267, 1671, 39404, 17158]\n",
      "Ridge regression: train loss is  0.9086268056269212\n",
      "[13044, 4975, 118253, 51228]\n",
      "[4290, 1645, 39460, 17105]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90857132\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90861357\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "28.354545454545455\n",
      "Ridge regression: train loss is  0.9089607536910893\n",
      "[13034, 5040, 118126, 51300]\n",
      "[4343, 1646, 39521, 16990]\n",
      "Ridge regression: train loss is  0.908676077656171\n",
      "[12819, 4897, 118450, 51334]\n",
      "[4403, 1660, 39326, 17111]\n",
      "Ridge regression: train loss is  0.9081252725823191\n",
      "[13016, 4915, 118343, 51226]\n",
      "[4260, 1660, 39415, 17165]\n",
      "Ridge regression: train loss is  0.9086614612026191\n",
      "[13008, 4953, 118275, 51264]\n",
      "[4280, 1641, 39464, 17115]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90860589\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90864791\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.32\n",
      "29.363636363636363\n",
      "Ridge regression: train loss is  0.9089949631943058\n",
      "[12996, 5008, 118158, 51338]\n",
      "[4329, 1641, 39526, 17004]\n",
      "Ridge regression: train loss is  0.9087105527415631\n",
      "[12791, 4866, 118481, 51362]\n",
      "[4389, 1652, 39334, 17125]\n",
      "Ridge regression: train loss is  0.9081608881024276\n",
      "[12977, 4893, 118365, 51265]\n",
      "[4245, 1652, 39423, 17180]\n",
      "Ridge regression: train loss is  0.9086963502864428\n",
      "[12972, 4921, 118307, 51300]\n",
      "[4270, 1628, 39477, 17125]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90864069\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90868247\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.32\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "30.37272727272727\n",
      "Ridge regression: train loss is  0.909029400011328\n",
      "[12959, 4975, 118191, 51375]\n",
      "[4320, 1631, 39536, 17013]\n",
      "Ridge regression: train loss is  0.908745238876921\n",
      "[12755, 4836, 118511, 51398]\n",
      "[4379, 1647, 39339, 17135]\n",
      "Ridge regression: train loss is  0.9081967044099286\n",
      "[12949, 4870, 118388, 51293]\n",
      "[4237, 1642, 39433, 17188]\n",
      "Ridge regression: train loss is  0.9087314644124123\n",
      "[12947, 4903, 118325, 51325]\n",
      "[4251, 1616, 39489, 17144]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90867570\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90871726\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "31.381818181818183\n",
      "Ridge regression: train loss is  0.9090640560066666\n",
      "[12920, 4946, 118220, 51414]\n",
      "[4309, 1625, 39542, 17024]\n",
      "Ridge regression: train loss is  0.9087801289124526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12719, 4807, 118540, 51434]\n",
      "[4363, 1637, 39349, 17151]\n",
      "Ridge regression: train loss is  0.9082327143790666\n",
      "[12910, 4842, 118416, 51332]\n",
      "[4226, 1630, 39445, 17199]\n",
      "Ridge regression: train loss is  0.9087667952611211\n",
      "[12918, 4878, 118350, 51354]\n",
      "[4240, 1603, 39502, 17155]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90871092\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90875226\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "32.39090909090909\n",
      "Ridge regression: train loss is  0.9090989231744849\n",
      "[12883, 4928, 118238, 51451]\n",
      "[4297, 1621, 39546, 17036]\n",
      "Ridge regression: train loss is  0.9088152157684085\n",
      "[12683, 4777, 118570, 51470]\n",
      "[4353, 1629, 39357, 17161]\n",
      "Ridge regression: train loss is  0.9082689109685808\n",
      "[12879, 4810, 118448, 51363]\n",
      "[4214, 1620, 39455, 17211]\n",
      "Ridge regression: train loss is  0.9088023346890094\n",
      "[12877, 4852, 118376, 51395]\n",
      "[4229, 1597, 39508, 17166]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90874635\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90878747\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "33.4\n",
      "Ridge regression: train loss is  0.909133993667261\n",
      "[12863, 4902, 118264, 51471]\n",
      "[4284, 1607, 39560, 17049]\n",
      "Ridge regression: train loss is  0.9088504924660715\n",
      "[12650, 4749, 118598, 51503]\n",
      "[4340, 1620, 39366, 17174]\n",
      "Ridge regression: train loss is  0.9083052872508596\n",
      "[12842, 4789, 118469, 51400]\n",
      "[4202, 1610, 39465, 17223]\n",
      "Ridge regression: train loss is  0.9088380747476417\n",
      "[12841, 4835, 118393, 51431]\n",
      "[4219, 1591, 39514, 17176]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90878196\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90882288\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "34.40909090909091\n",
      "Ridge regression: train loss is  0.9091692598152887\n",
      "[12835, 4873, 118293, 51499]\n",
      "[4263, 1596, 39571, 17070]\n",
      "Ridge regression: train loss is  0.9088859521498627\n",
      "[12610, 4723, 118624, 51543]\n",
      "[4328, 1618, 39368, 17186]\n",
      "Ridge regression: train loss is  0.9083418364323879\n",
      "[12808, 4759, 118499, 51434]\n",
      "[4194, 1600, 39475, 17231]\n",
      "Ridge regression: train loss is  0.9088740076956535\n",
      "[12791, 4814, 118414, 51481]\n",
      "[4208, 1585, 39520, 17187]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90881776\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90885847\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "35.418181818181814\n",
      "Ridge regression: train loss is  0.9092047141393518\n",
      "[12810, 4854, 118312, 51524]\n",
      "[4255, 1589, 39578, 17078]\n",
      "Ridge regression: train loss is  0.9089215881027657\n",
      "[12583, 4693, 118654, 51570]\n",
      "[4312, 1611, 39375, 17202]\n",
      "Ridge regression: train loss is  0.9083785518676838\n",
      "[12763, 4738, 118520, 51479]\n",
      "[4185, 1591, 39484, 17240]\n",
      "Ridge regression: train loss is  0.9089101260053334\n",
      "[12755, 4791, 118437, 51517]\n",
      "[4197, 1571, 39534, 17198]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90885375\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90889426\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "36.42727272727273\n",
      "Ridge regression: train loss is  0.9092403493583027\n",
      "[12768, 4829, 118337, 51566]\n",
      "[4249, 1584, 39583, 17084]\n",
      "Ridge regression: train loss is  0.9089573937567195\n",
      "[12555, 4667, 118680, 51598]\n",
      "[4299, 1604, 39382, 17215]\n",
      "Ridge regression: train loss is  0.9084154270683621\n",
      "[12719, 4711, 118547, 51523]\n",
      "[4171, 1577, 39498, 17254]\n",
      "Ridge regression: train loss is  0.9089464223652777\n",
      "[12723, 4766, 118462, 51549]\n",
      "[4188, 1556, 39549, 17207]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90888990\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90893022\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "37.43636363636364\n",
      "Ridge regression: train loss is  0.9092761583928506\n",
      "[12729, 4797, 118369, 51605]\n",
      "[4238, 1579, 39588, 17095]\n",
      "Ridge regression: train loss is  0.908993362699213\n",
      "[12522, 4647, 118700, 51631]\n",
      "[4290, 1600, 39386, 17224]\n",
      "Ridge regression: train loss is  0.9084524557085466\n",
      "[12684, 4695, 118563, 51558]\n",
      "[4160, 1568, 39507, 17265]\n",
      "Ridge regression: train loss is  0.908982889680204\n",
      "[12692, 4738, 118490, 51580]\n",
      "[4175, 1552, 39553, 17220]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90892622\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90896634\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "38.445454545454545\n",
      "Ridge regression: train loss is  0.9093121343665366\n",
      "[12695, 4767, 118399, 51639]\n",
      "[4223, 1573, 39594, 17110]\n",
      "Ridge regression: train loss is  0.9090294886770155\n",
      "[12478, 4626, 118721, 51675]\n",
      "[4282, 1589, 39397, 17232]\n",
      "Ridge regression: train loss is  0.9084896316275543\n",
      "[12651, 4669, 118589, 51591]\n",
      "[4146, 1559, 39516, 17279]\n",
      "Ridge regression: train loss is  0.9090195210687098\n",
      "[12652, 4710, 118518, 51620]\n",
      "[4161, 1547, 39558, 17234]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90896269\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90900264\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "39.45454545454545\n",
      "Ridge regression: train loss is  0.9093482706046376\n",
      "[12664, 4745, 118421, 51670]\n",
      "[4214, 1569, 39598, 17119]\n",
      "Ridge regression: train loss is  0.9090657655977532\n",
      "[12440, 4611, 118736, 51713]\n",
      "[4269, 1582, 39404, 17245]\n",
      "Ridge regression: train loss is  0.9085269488305444\n",
      "[12614, 4658, 118600, 51628]\n",
      "[4137, 1550, 39525, 17288]\n",
      "Ridge regression: train loss is  0.9090563098595761\n",
      "[12620, 4680, 118548, 51652]\n",
      "[4148, 1538, 39567, 17247]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90899932\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90903908\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "40.46363636363636\n",
      "Ridge regression: train loss is  0.9093845606315579\n",
      "[12636, 4722, 118444, 51698]\n",
      "[4208, 1555, 39612, 17125]\n",
      "Ridge regression: train loss is  0.9091021875298687\n",
      "[12409, 4593, 118754, 51744]\n",
      "[4259, 1576, 39410, 17255]\n",
      "Ridge regression: train loss is  0.9085644014876622\n",
      "[12591, 4627, 118631, 51651]\n",
      "[4130, 1542, 39533, 17295]\n",
      "Ridge regression: train loss is  0.9090932495870637\n",
      "[12585, 4662, 118566, 51687]\n",
      "[4139, 1529, 39576, 17256]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90903610\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90907568\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "41.47272727272727\n",
      "Ridge regression: train loss is  0.9094209981671313\n",
      "[12606, 4691, 118475, 51728]\n",
      "[4199, 1545, 39622, 17134]\n",
      "Ridge regression: train loss is  0.9091387487013768\n",
      "[12371, 4567, 118780, 51782]\n",
      "[4248, 1576, 39410, 17266]\n",
      "Ridge regression: train loss is  0.9086019839320711\n",
      "[12561, 4610, 118648, 51681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4114, 1535, 39540, 17311]\n",
      "Ridge regression: train loss is  0.9091303339855228\n",
      "[12562, 4649, 118579, 51710]\n",
      "[4129, 1518, 39587, 17266]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90907302\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90911243\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "42.481818181818184\n",
      "Ridge regression: train loss is  0.9094575771221618\n",
      "[12565, 4672, 118494, 51769]\n",
      "[4191, 1533, 39634, 17142]\n",
      "Ridge regression: train loss is  0.9091754434977318\n",
      "[12344, 4543, 118804, 51809]\n",
      "[4240, 1569, 39417, 17274]\n",
      "Ridge regression: train loss is  0.9086396906571879\n",
      "[12522, 4587, 118671, 51720]\n",
      "[4098, 1526, 39549, 17327]\n",
      "Ridge regression: train loss is  0.9091675569835753\n",
      "[12533, 4621, 118607, 51739]\n",
      "[4115, 1516, 39589, 17280]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90911007\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90914931\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "43.49090909090909\n",
      "Ridge regression: train loss is  0.9094942915934398\n",
      "[12533, 4645, 118521, 51801]\n",
      "[4177, 1532, 39635, 17156]\n",
      "Ridge regression: train loss is  0.9092122664590467\n",
      "[12310, 4520, 118827, 51843]\n",
      "[4235, 1563, 39423, 17279]\n",
      "Ridge regression: train loss is  0.9086775163133418\n",
      "[12490, 4573, 118685, 51752]\n",
      "[4083, 1520, 39555, 17342]\n",
      "Ridge regression: train loss is  0.9092049126980448\n",
      "[12501, 4601, 118627, 51771]\n",
      "[4102, 1512, 39593, 17293]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90914725\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90918632\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "44.5\n",
      "Ridge regression: train loss is  0.9095311358584293\n",
      "[12493, 4631, 118535, 51841]\n",
      "[4168, 1520, 39647, 17165]\n",
      "Ridge regression: train loss is  0.909249212276852\n",
      "[12279, 4492, 118855, 51874]\n",
      "[4223, 1558, 39428, 17291]\n",
      "Ridge regression: train loss is  0.9087154557040429\n",
      "[12461, 4549, 118709, 51781]\n",
      "[4073, 1515, 39560, 17352]\n",
      "Ridge regression: train loss is  0.9092423954277737\n",
      "[12459, 4578, 118650, 51813]\n",
      "[4090, 1499, 39606, 17305]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90918455\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90922346\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "45.50909090909091\n",
      "Ridge regression: train loss is  0.9095681043697579\n",
      "[12468, 4612, 118554, 51866]\n",
      "[4158, 1513, 39654, 17175]\n",
      "Ridge regression: train loss is  0.9092862757905341\n",
      "[12242, 4470, 118877, 51911]\n",
      "[4217, 1553, 39433, 17297]\n",
      "Ridge regression: train loss is  0.9087535037819922\n",
      "[12426, 4536, 118722, 51816]\n",
      "[4061, 1502, 39573, 17364]\n",
      "Ridge regression: train loss is  0.9092799996474282\n",
      "[12417, 4552, 118676, 51855]\n",
      "[4085, 1495, 39610, 17310]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90922197\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90926072\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.31\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.31\n",
      "46.518181818181816\n",
      "Ridge regression: train loss is  0.9096051917496205\n",
      "[12437, 4598, 118568, 51897]\n",
      "[4145, 1506, 39661, 17188]\n",
      "Ridge regression: train loss is  0.9093234519835685\n",
      "[12208, 4454, 118893, 51945]\n",
      "[4203, 1550, 39436, 17311]\n",
      "Ridge regression: train loss is  0.9087916556449327\n",
      "[12397, 4516, 118742, 51845]\n",
      "[4049, 1498, 39577, 17376]\n",
      "Ridge regression: train loss is  0.9093177200013616\n",
      "[12386, 4537, 118691, 51886]\n",
      "[4071, 1486, 39619, 17324]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90925950\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90929809\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "47.527272727272724\n",
      "Ridge regression: train loss is  0.9096423927841737\n",
      "[12399, 4578, 118588, 51935]\n",
      "[4138, 1499, 39668, 17195]\n",
      "Ridge regression: train loss is  0.9093607359796219\n",
      "[12172, 4432, 118915, 51981]\n",
      "[4196, 1541, 39445, 17318]\n",
      "Ridge regression: train loss is  0.908829906531425\n",
      "[12356, 4496, 118762, 51886]\n",
      "[4043, 1493, 39582, 17382]\n",
      "Ridge regression: train loss is  0.9093555512975914\n",
      "[12351, 4525, 118703, 51921]\n",
      "[4056, 1482, 39623, 17339]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90929715\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90933558\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "48.53636363636364\n",
      "Ridge regression: train loss is  0.9096797024179828\n",
      "[12362, 4570, 118596, 51972]\n",
      "[4129, 1491, 39676, 17204]\n",
      "Ridge regression: train loss is  0.9093981230386007\n",
      "[12137, 4416, 118931, 52016]\n",
      "[4183, 1535, 39451, 17331]\n",
      "Ridge regression: train loss is  0.908868251816601\n",
      "[12313, 4480, 118778, 51929]\n",
      "[4036, 1489, 39586, 17389]\n",
      "Ridge regression: train loss is  0.9093934885019238\n",
      "[12320, 4512, 118716, 51952]\n",
      "[4046, 1481, 39624, 17349]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90933489\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90937317\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "49.54545454545455\n",
      "Ridge regression: train loss is  0.9097171157485606\n",
      "[12321, 4547, 118619, 52013]\n",
      "[4115, 1484, 39683, 17218]\n",
      "Ridge regression: train loss is  0.9094356085526804\n",
      "[12100, 4399, 118948, 52053]\n",
      "[4168, 1529, 39457, 17346]\n",
      "Ridge regression: train loss is  0.9089066870079432\n",
      "[12281, 4463, 118795, 51961]\n",
      "[4028, 1487, 39588, 17397]\n",
      "Ridge regression: train loss is  0.909431526732253\n",
      "[12290, 4493, 118735, 51982]\n",
      "[4034, 1476, 39629, 17361]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90937273\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90941086\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "50.554545454545455\n",
      "Ridge regression: train loss is  0.9097546280210315\n",
      "[12287, 4532, 118634, 52047]\n",
      "[4104, 1478, 39689, 17229]\n",
      "Ridge regression: train loss is  0.9094731880423642\n",
      "[12065, 4381, 118966, 52088]\n",
      "[4155, 1518, 39468, 17359]\n",
      "Ridge regression: train loss is  0.9089452077411195\n",
      "[12240, 4444, 118814, 52002]\n",
      "[4017, 1482, 39593, 17408]\n",
      "Ridge regression: train loss is  0.9094696612530487\n",
      "[12254, 4475, 118753, 52018]\n",
      "[4027, 1467, 39638, 17368]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90941067\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90944865\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "51.56363636363636\n",
      "Ridge regression: train loss is  0.9097922346229425\n",
      "[12266, 4507, 118659, 52068]\n",
      "[4090, 1475, 39692, 17243]\n",
      "Ridge regression: train loss is  0.9095108571525922\n",
      "[12030, 4366, 118981, 52123]\n",
      "[4148, 1511, 39475, 17366]\n",
      "Ridge regression: train loss is  0.9089838097758998\n",
      "[12207, 4428, 118830, 52035]\n",
      "[4002, 1476, 39599, 17423]\n",
      "Ridge regression: train loss is  0.909507887470045\n",
      "[12220, 4461, 118767, 52052]\n",
      "[4019, 1460, 39645, 17376]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90944870\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90948653\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.07\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "52.57272727272727\n",
      "Ridge regression: train loss is  0.9098299310792325\n",
      "[12235, 4493, 118673, 52099]\n",
      "[4071, 1468, 39699, 17262]\n",
      "Ridge regression: train loss is  0.9095486116489222\n",
      "[11997, 4352, 118995, 52156]\n",
      "[4132, 1502, 39484, 17382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: train loss is  0.9090224889921682\n",
      "[12179, 4405, 118853, 52063]\n",
      "[3992, 1472, 39603, 17433]\n",
      "Ridge regression: train loss is  0.9095462009251306\n",
      "[12182, 4441, 118787, 52090]\n",
      "[4013, 1450, 39655, 17382]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90948681\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90952450\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "53.58181818181818\n",
      "Ridge regression: train loss is  0.9098677130473765\n",
      "[12201, 4470, 118696, 52133]\n",
      "[4059, 1465, 39702, 17274]\n",
      "Ridge regression: train loss is  0.9095864474138002\n",
      "[11961, 4335, 119012, 52192]\n",
      "[4115, 1496, 39490, 17399]\n",
      "Ridge regression: train loss is  0.9090612413860464\n",
      "[12148, 4383, 118875, 52094]\n",
      "[3985, 1466, 39609, 17440]\n",
      "Ridge regression: train loss is  0.9095845972914443\n",
      "[12151, 4425, 118803, 52121]\n",
      "[4009, 1441, 39664, 17386]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90952500\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90956255\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "54.590909090909086\n",
      "Ridge regression: train loss is  0.9099055763126992\n",
      "[12169, 4454, 118712, 52165]\n",
      "[4042, 1461, 39706, 17291]\n",
      "Ridge regression: train loss is  0.9096243604429259\n",
      "[11932, 4321, 119026, 52221]\n",
      "[4094, 1491, 39495, 17420]\n",
      "Ridge regression: train loss is  0.9091000630661324\n",
      "[12119, 4362, 118896, 52123]\n",
      "[3974, 1456, 39619, 17451]\n",
      "Ridge regression: train loss is  0.9096230723686727\n",
      "[12111, 4403, 118825, 52161]\n",
      "[4001, 1428, 39677, 17394]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90956327\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90960068\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "55.6\n",
      "Ridge regression: train loss is  0.9099435167838675\n",
      "[12130, 4427, 118739, 52204]\n",
      "[4028, 1458, 39709, 17305]\n",
      "Ridge regression: train loss is  0.9096623468417214\n",
      "[11908, 4304, 119043, 52245]\n",
      "[4076, 1487, 39499, 17438]\n",
      "Ridge regression: train loss is  0.9091389502498608\n",
      "[12089, 4340, 118918, 52153]\n",
      "[3965, 1451, 39624, 17460]\n",
      "Ridge regression: train loss is  0.9096616220785457\n",
      "[12080, 4382, 118846, 52192]\n",
      "[3995, 1423, 39682, 17400]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90960161\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90963889\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "56.60909090909091\n",
      "Ridge regression: train loss is  0.9099815304885591\n",
      "[12090, 4412, 118754, 52244]\n",
      "[4016, 1450, 39717, 17317]\n",
      "Ridge regression: train loss is  0.9097004028219126\n",
      "[11884, 4293, 119054, 52269]\n",
      "[4066, 1483, 39503, 17448]\n",
      "Ridge regression: train loss is  0.9091778992599869\n",
      "[12063, 4324, 118934, 52179]\n",
      "[3959, 1445, 39630, 17466]\n",
      "Ridge regression: train loss is  0.909700242460525\n",
      "[12050, 4368, 118860, 52222]\n",
      "[3984, 1418, 39687, 17411]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90964002\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90967716\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "57.61818181818182\n",
      "Ridge regression: train loss is  0.9100196135693029\n",
      "[12063, 4395, 118771, 52271]\n",
      "[4006, 1445, 39722, 17327]\n",
      "Ridge regression: train loss is  0.9097385246982161\n",
      "[11864, 4277, 119070, 52289]\n",
      "[4059, 1474, 39512, 17455]\n",
      "Ridge regression: train loss is  0.9092169065211951\n",
      "[12033, 4301, 118957, 52209]\n",
      "[3953, 1440, 39635, 17472]\n",
      "Ridge regression: train loss is  0.9097389296676784\n",
      "[12010, 4356, 118872, 52262]\n",
      "[3978, 1412, 39693, 17417]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90967849\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90971550\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "58.627272727272725\n",
      "Ridge regression: train loss is  0.9100577622794909\n",
      "[12036, 4371, 118795, 52298]\n",
      "[3995, 1442, 39725, 17338]\n",
      "Ridge regression: train loss is  0.909776708885141\n",
      "[11833, 4254, 119093, 52320]\n",
      "[4051, 1467, 39519, 17463]\n",
      "Ridge regression: train loss is  0.9092559685568288\n",
      "[12009, 4284, 118974, 52233]\n",
      "[3939, 1433, 39642, 17486]\n",
      "Ridge regression: train loss is  0.9097776799627352\n",
      "[11983, 4345, 118883, 52289]\n",
      "[3975, 1409, 39696, 17420]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90971703\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90975391\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "59.63636363636363\n",
      "Ridge regression: train loss is  0.9100959729795512\n",
      "[12005, 4357, 118809, 52329]\n",
      "[3979, 1436, 39731, 17354]\n",
      "Ridge regression: train loss is  0.9098149518938979\n",
      "[11809, 4236, 119111, 52344]\n",
      "[4042, 1456, 39530, 17472]\n",
      "Ridge regression: train loss is  0.9092950819857425\n",
      "[11984, 4269, 118989, 52258]\n",
      "[3927, 1426, 39649, 17498]\n",
      "Ridge regression: train loss is  0.9098164897143114\n",
      "[11952, 4325, 118903, 52320]\n",
      "[3970, 1400, 39705, 17425]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90975562\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90979238\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "60.64545454545454\n",
      "Ridge regression: train loss is  0.9101342421332853\n",
      "[11980, 4340, 118826, 52354]\n",
      "[3971, 1430, 39737, 17362]\n",
      "Ridge regression: train loss is  0.9098532503294205\n",
      "[11782, 4219, 119128, 52371]\n",
      "[4030, 1454, 39532, 17484]\n",
      "Ridge regression: train loss is  0.9093342435192733\n",
      "[11960, 4247, 119011, 52282]\n",
      "[3914, 1420, 39655, 17511]\n",
      "Ridge regression: train loss is  0.9098553553933039\n",
      "[11915, 4304, 118924, 52357]\n",
      "[3963, 1396, 39709, 17432]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90979427\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90983090\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "61.654545454545456\n",
      "Ridge regression: train loss is  0.9101725663043552\n",
      "[11950, 4322, 118844, 52384]\n",
      "[3962, 1427, 39740, 17371]\n",
      "Ridge regression: train loss is  0.909891600887489\n",
      "[11748, 4201, 119146, 52405]\n",
      "[4017, 1448, 39538, 17497]\n",
      "Ridge regression: train loss is  0.9093734499583239\n",
      "[11929, 4230, 119028, 52313]\n",
      "[3899, 1416, 39659, 17526]\n",
      "Ridge regression: train loss is  0.9098942735694358\n",
      "[11882, 4279, 118949, 52390]\n",
      "[3953, 1389, 39716, 17442]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90983297\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90986947\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.30\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.30\n",
      "62.663636363636364\n",
      "Ridge regression: train loss is  0.9102109421529195\n",
      "[11927, 4301, 118865, 52407]\n",
      "[3952, 1422, 39745, 17381]\n",
      "Ridge regression: train loss is  0.909930000351964\n",
      "[11718, 4175, 119172, 52435]\n",
      "[4005, 1441, 39545, 17509]\n",
      "Ridge regression: train loss is  0.9094126981905594\n",
      "[11898, 4211, 119047, 52344]\n",
      "[3887, 1414, 39661, 17538]\n",
      "Ridge regression: train loss is  0.9099332409079589\n",
      "[11850, 4266, 118962, 52422]\n",
      "[3942, 1383, 39722, 17453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged train rmse after 4-fold cross-validation = 0.90987172\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90990810\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "63.67272727272727\n",
      "Ridge regression: train loss is  0.9102493664324126\n",
      "[11897, 4292, 118874, 52437]\n",
      "[3948, 1417, 39750, 17385]\n",
      "Ridge regression: train loss is  0.9099684455921155\n",
      "[11684, 4149, 119198, 52469]\n",
      "[3997, 1437, 39549, 17517]\n",
      "Ridge regression: train loss is  0.9094519851877125\n",
      "[11875, 4192, 119066, 52367]\n",
      "[3875, 1406, 39669, 17550]\n",
      "Ridge regression: train loss is  0.909972254166493\n",
      "[11822, 4251, 118977, 52450]\n",
      "[3932, 1378, 39727, 17463]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90991051\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90994677\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "64.68181818181817\n",
      "Ridge regression: train loss is  0.9102878359864565\n",
      "[11876, 4275, 118891, 52458]\n",
      "[3939, 1412, 39755, 17394]\n",
      "Ridge regression: train loss is  0.9100069335600537\n",
      "[11648, 4136, 119211, 52505]\n",
      "[3987, 1434, 39552, 17527]\n",
      "Ridge regression: train loss is  0.9094913080029898\n",
      "[11849, 4176, 119082, 52393]\n",
      "[3870, 1401, 39674, 17555]\n",
      "Ridge regression: train loss is  0.9100113101920053\n",
      "[11795, 4235, 118993, 52477]\n",
      "[3923, 1363, 39742, 17472]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90994935\n",
      "Averaged test rmse after 4-fold cross-validation = 0.90998548\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "65.69090909090909\n",
      "Ridge regression: train loss is  0.9103263477459052\n",
      "[11849, 4250, 118916, 52485]\n",
      "[3931, 1407, 39760, 17402]\n",
      "Ridge regression: train loss is  0.9100454612882511\n",
      "[11633, 4115, 119232, 52520]\n",
      "[3972, 1432, 39554, 17542]\n",
      "Ridge regression: train loss is  0.9095306637685806\n",
      "[11814, 4154, 119104, 52428]\n",
      "[3855, 1396, 39679, 17570]\n",
      "Ridge regression: train loss is  0.9100504059179143\n",
      "[11767, 4217, 119011, 52505]\n",
      "[3914, 1358, 39747, 17481]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.90998822\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91002424\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "66.69999999999999\n",
      "Ridge regression: train loss is  0.9103648987260099\n",
      "[11818, 4225, 118941, 52516]\n",
      "[3921, 1402, 39765, 17412]\n",
      "Ridge regression: train loss is  0.9100840258871619\n",
      "[11607, 4089, 119258, 52546]\n",
      "[3966, 1429, 39557, 17548]\n",
      "Ridge regression: train loss is  0.9095700496932578\n",
      "[11773, 4132, 119126, 52469]\n",
      "[3847, 1389, 39686, 17578]\n",
      "Ridge regression: train loss is  0.9100895383613222\n",
      "[11734, 4197, 119031, 52538]\n",
      "[3909, 1353, 39752, 17486]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91002713\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91006303\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "67.7090909090909\n",
      "Ridge regression: train loss is  0.9104034860237054\n",
      "[11794, 4205, 118961, 52540]\n",
      "[3917, 1395, 39772, 17416]\n",
      "Ridge regression: train loss is  0.9101226245429181\n",
      "[11580, 4073, 119274, 52573]\n",
      "[3954, 1424, 39562, 17560]\n",
      "Ridge regression: train loss is  0.9096094630600725\n",
      "[11745, 4116, 119142, 52497]\n",
      "[3837, 1382, 39693, 17588]\n",
      "Ridge regression: train loss is  0.9101287046203582\n",
      "[11704, 4185, 119043, 52568]\n",
      "[3899, 1347, 39758, 17496]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91006607\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91010185\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "68.7181818181818\n",
      "Ridge regression: train loss is  0.9104421068150046\n",
      "[11756, 4186, 118980, 52578]\n",
      "[3907, 1389, 39778, 17426]\n",
      "Ridge regression: train loss is  0.9101612545151229\n",
      "[11566, 4047, 119300, 52587]\n",
      "[3943, 1418, 39568, 17571]\n",
      "Ridge regression: train loss is  0.9096489012241377\n",
      "[11712, 4100, 119158, 52530]\n",
      "[3827, 1376, 39699, 17598]\n",
      "Ridge regression: train loss is  0.9101679018716341\n",
      "[11675, 4169, 119059, 52597]\n",
      "[3887, 1342, 39763, 17508]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91010504\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91014071\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "69.72727272727272\n",
      "Ridge regression: train loss is  0.9104807583525039\n",
      "[11734, 4174, 118992, 52600]\n",
      "[3900, 1383, 39784, 17433]\n",
      "Ridge regression: train loss is  0.9101999131347139\n",
      "[11534, 4037, 119310, 52619]\n",
      "[3931, 1409, 39577, 17583]\n",
      "Ridge regression: train loss is  0.9096883616104922\n",
      "[11684, 4080, 119178, 52558]\n",
      "[3823, 1368, 39707, 17602]\n",
      "Ridge regression: train loss is  0.9102071273678066\n",
      "[11644, 4152, 119076, 52628]\n",
      "[3880, 1335, 39770, 17515]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91014404\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91017960\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "70.73636363636362\n",
      "Ridge regression: train loss is  0.9105194379629875\n",
      "[11708, 4155, 119011, 52626]\n",
      "[3893, 1371, 39796, 17440]\n",
      "Ridge regression: train loss is  0.9102385978019101\n",
      "[11510, 4025, 119322, 52643]\n",
      "[3919, 1404, 39582, 17595]\n",
      "Ridge regression: train loss is  0.9097278417120478\n",
      "[11659, 4072, 119186, 52583]\n",
      "[3815, 1363, 39712, 17610]\n",
      "Ridge regression: train loss is  0.9102463784352364\n",
      "[11622, 4138, 119090, 52650]\n",
      "[3868, 1330, 39775, 17527]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91018306\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91021851\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "71.74545454545454\n",
      "Ridge regression: train loss is  0.9105581430451298\n",
      "[11671, 4137, 119029, 52663]\n",
      "[3885, 1363, 39804, 17448]\n",
      "Ridge regression: train loss is  0.9102773059842312\n",
      "[11483, 4003, 119344, 52670]\n",
      "[3914, 1399, 39587, 17600]\n",
      "Ridge regression: train loss is  0.9097673390876114\n",
      "[11631, 4054, 119204, 52611]\n",
      "[3809, 1356, 39719, 17616]\n",
      "Ridge regression: train loss is  0.9102856524717419\n",
      "[11606, 4116, 119112, 52666]\n",
      "[3861, 1328, 39777, 17534]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91022211\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91025744\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "72.75454545454545\n",
      "Ridge regression: train loss is  0.9105968710672904\n",
      "[11649, 4128, 119038, 52685]\n",
      "[3877, 1360, 39807, 17456]\n",
      "Ridge regression: train loss is  0.9103160352145877\n",
      "[11457, 3992, 119355, 52696]\n",
      "[3908, 1393, 39593, 17606]\n",
      "Ridge regression: train loss is  0.9098068513599815\n",
      "[11606, 4036, 119222, 52636]\n",
      "[3805, 1352, 39723, 17620]\n",
      "Ridge regression: train loss is  0.9103249469444451\n",
      "[11585, 4088, 119140, 52687]\n",
      "[3850, 1323, 39782, 17545]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91026118\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91029640\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "73.76363636363635\n",
      "Ridge regression: train loss is  0.9106356195653948\n",
      "[11623, 4109, 119057, 52711]\n",
      "[3865, 1356, 39811, 17468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: train loss is  0.9103547830894396\n",
      "[11433, 3976, 119371, 52720]\n",
      "[3898, 1387, 39599, 17616]\n",
      "Ridge regression: train loss is  0.9098463762141134\n",
      "[11582, 4021, 119237, 52660]\n",
      "[3799, 1348, 39727, 17626]\n",
      "Ridge regression: train loss is  0.9103642593876996\n",
      "[11552, 4070, 119158, 52720]\n",
      "[3842, 1319, 39786, 17553]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91030026\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91033538\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "74.77272727272727\n",
      "Ridge regression: train loss is  0.910674386140901\n",
      "[11601, 4102, 119064, 52733]\n",
      "[3862, 1352, 39815, 17471]\n",
      "Ridge regression: train loss is  0.9103935472670198\n",
      "[11395, 3957, 119390, 52758]\n",
      "[3890, 1384, 39602, 17624]\n",
      "Ridge regression: train loss is  0.9098859113953529\n",
      "[11555, 4010, 119248, 52687]\n",
      "[3785, 1342, 39733, 17640]\n",
      "Ridge regression: train loss is  0.9104035874011007\n",
      "[11523, 4054, 119174, 52749]\n",
      "[3832, 1318, 39787, 17563]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91033936\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91037437\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "75.78181818181817\n",
      "Ridge regression: train loss is  0.9107131684588433\n",
      "[11579, 4082, 119084, 52755]\n",
      "[3854, 1349, 39818, 17479]\n",
      "Ridge regression: train loss is  0.9104323254656211\n",
      "[11373, 3941, 119406, 52780]\n",
      "[3882, 1379, 39607, 17632]\n",
      "Ridge regression: train loss is  0.9099254547077315\n",
      "[11534, 3992, 119266, 52708]\n",
      "[3777, 1338, 39737, 17648]\n",
      "Ridge regression: train loss is  0.9104429286475733\n",
      "[11492, 4039, 119189, 52780]\n",
      "[3822, 1311, 39794, 17573]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91037847\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91041338\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "76.79090909090908\n",
      "Ridge regression: train loss is  0.9107519642459531\n",
      "[11562, 4073, 119093, 52772]\n",
      "[3844, 1341, 39826, 17489]\n",
      "Ridge regression: train loss is  0.9104711154619409\n",
      "[11347, 3929, 119418, 52806]\n",
      "[3876, 1372, 39614, 17638]\n",
      "Ridge regression: train loss is  0.909965004012325\n",
      "[11509, 3980, 119278, 52733]\n",
      "[3771, 1331, 39744, 17654]\n",
      "Ridge regression: train loss is  0.91048228085153\n",
      "[11470, 4020, 119208, 52802]\n",
      "[3814, 1306, 39799, 17581]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91041759\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91045239\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "77.8\n",
      "Ridge regression: train loss is  0.9107907712888497\n",
      "[11525, 4051, 119115, 52809]\n",
      "[3840, 1336, 39831, 17493]\n",
      "Ridge regression: train loss is  0.9105099150894853\n",
      "[11326, 3913, 119434, 52827]\n",
      "[3865, 1367, 39619, 17649]\n",
      "Ridge regression: train loss is  0.910004557225669\n",
      "[11491, 3965, 119293, 52751]\n",
      "[3765, 1322, 39753, 17660]\n",
      "Ridge regression: train loss is  0.9105216417971022\n",
      "[11443, 3999, 119229, 52829]\n",
      "[3804, 1303, 39802, 17591]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91045672\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91049142\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "78.8090909090909\n",
      "Ridge regression: train loss is  0.9108295874323021\n",
      "[11506, 4022, 119144, 52828]\n",
      "[3831, 1331, 39836, 17502]\n",
      "Ridge regression: train loss is  0.910548722237026\n",
      "[11300, 3902, 119445, 52853]\n",
      "[3861, 1356, 39630, 17653]\n",
      "Ridge regression: train loss is  0.9100441123182293\n",
      "[11471, 3950, 119308, 52771]\n",
      "[3753, 1321, 39754, 17672]\n",
      "Ridge regression: train loss is  0.910561009326434\n",
      "[11418, 3986, 119242, 52854]\n",
      "[3796, 1296, 39809, 17599]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91049586\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91053046\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "79.81818181818181\n",
      "Ridge regression: train loss is  0.9108684105775542\n",
      "[11473, 4004, 119162, 52861]\n",
      "[3818, 1325, 39842, 17515]\n",
      "Ridge regression: train loss is  0.9105875348471107\n",
      "[11273, 3894, 119453, 52880]\n",
      "[3850, 1354, 39632, 17664]\n",
      "Ridge regression: train loss is  0.9100836673129279\n",
      "[11450, 3940, 119318, 52792]\n",
      "[3741, 1318, 39757, 17684]\n",
      "Ridge regression: train loss is  0.9106003813380418\n",
      "[11395, 3975, 119253, 52877]\n",
      "[3784, 1289, 39816, 17611]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91053500\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91056950\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "80.82727272727271\n",
      "Ridge regression: train loss is  0.9109072386807108\n",
      "[11452, 3994, 119172, 52882]\n",
      "[3810, 1321, 39846, 17523]\n",
      "Ridge regression: train loss is  0.9106263509146237\n",
      "[11250, 3884, 119463, 52903]\n",
      "[3841, 1347, 39639, 17673]\n",
      "Ridge regression: train loss is  0.9101232202837167\n",
      "[11425, 3923, 119335, 52817]\n",
      "[3733, 1313, 39762, 17692]\n",
      "Ridge regression: train loss is  0.9106397557852303\n",
      "[11372, 3959, 119269, 52900]\n",
      "[3776, 1287, 39818, 17619]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91057414\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91060854\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "81.83636363636363\n",
      "Ridge regression: train loss is  0.9109460697511869\n",
      "[11428, 3983, 119183, 52906]\n",
      "[3804, 1320, 39847, 17529]\n",
      "Ridge regression: train loss is  0.9106651684853935\n",
      "[11226, 3876, 119471, 52927]\n",
      "[3833, 1341, 39645, 17681]\n",
      "Ridge regression: train loss is  0.9101627693542035\n",
      "[11396, 3913, 119345, 52846]\n",
      "[3719, 1307, 39768, 17706]\n",
      "Ridge regression: train loss is  0.9106791306745703\n",
      "[11355, 3951, 119277, 52917]\n",
      "[3769, 1282, 39823, 17626]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91061328\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91064759\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.29\n",
      "82.84545454545453\n",
      "Ridge regression: train loss is  0.9109849018502079\n",
      "[11408, 3973, 119193, 52926]\n",
      "[3798, 1314, 39853, 17535]\n",
      "Ridge regression: train loss is  0.9107039856548467\n",
      "[11207, 3862, 119485, 52946]\n",
      "[3826, 1336, 39650, 17688]\n",
      "Ridge regression: train loss is  0.91020231269632\n",
      "[11373, 3899, 119359, 52869]\n",
      "[3710, 1299, 39776, 17715]\n",
      "Ridge regression: train loss is  0.9107185040644245\n",
      "[11337, 3935, 119293, 52935]\n",
      "[3758, 1277, 39828, 17637]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91065243\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91068663\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.29\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "83.85454545454544\n",
      "Ridge regression: train loss is  0.9110237330893686\n",
      "[11383, 3962, 119204, 52951]\n",
      "[3792, 1308, 39859, 17541]\n",
      "Ridge regression: train loss is  0.9107428005667075\n",
      "[11182, 3849, 119498, 52971]\n",
      "[3817, 1329, 39657, 17697]\n",
      "Ridge regression: train loss is  0.9102418485290391\n",
      "[11348, 3892, 119366, 52894]\n",
      "[3702, 1293, 39782, 17723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: train loss is  0.9107578740635324\n",
      "[11310, 3924, 119304, 52962]\n",
      "[3744, 1272, 39833, 17651]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91069156\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91072567\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "84.86363636363636\n",
      "Ridge regression: train loss is  0.9110625616292404\n",
      "[11362, 3948, 119218, 52972]\n",
      "[3783, 1307, 39860, 17550]\n",
      "Ridge regression: train loss is  0.9107816114117379\n",
      "[11149, 3837, 119510, 53004]\n",
      "[3811, 1326, 39660, 17703]\n",
      "Ridge regression: train loss is  0.910281375117131\n",
      "[11329, 3878, 119380, 52913]\n",
      "[3696, 1291, 39784, 17729]\n",
      "Ridge regression: train loss is  0.9107972388296376\n",
      "[11280, 3912, 119316, 52992]\n",
      "[3736, 1271, 39834, 17659]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91073070\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91076471\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "85.87272727272726\n",
      "Ridge regression: train loss is  0.9111013856780276\n",
      "[11342, 3932, 119234, 52992]\n",
      "[3773, 1305, 39862, 17560]\n",
      "Ridge regression: train loss is  0.9108204164265178\n",
      "[11123, 3824, 119523, 53030]\n",
      "[3801, 1326, 39660, 17713]\n",
      "Ridge regression: train loss is  0.9103208907699617\n",
      "[11300, 3866, 119392, 52942]\n",
      "[3690, 1285, 39790, 17735]\n",
      "Ridge regression: train loss is  0.9108365965681676\n",
      "[11251, 3900, 119328, 53021]\n",
      "[3729, 1267, 39838, 17666]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91076982\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91080374\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "86.88181818181818\n",
      "Ridge regression: train loss is  0.9111402034902717\n",
      "[11314, 3919, 119247, 53020]\n",
      "[3763, 1304, 39863, 17570]\n",
      "Ridge regression: train loss is  0.9108592138922659\n",
      "[11090, 3810, 119537, 53063]\n",
      "[3792, 1322, 39664, 17722]\n",
      "Ridge regression: train loss is  0.910360393840331\n",
      "[11287, 3855, 119403, 52955]\n",
      "[3682, 1281, 39794, 17743]\n",
      "Ridge regression: train loss is  0.9108759455309555\n",
      "[11227, 3887, 119341, 53045]\n",
      "[3722, 1263, 39842, 17673]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91080894\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91084276\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "87.89090909090908\n",
      "Ridge regression: train loss is  0.9111790133655999\n",
      "[11289, 3903, 119263, 53045]\n",
      "[3757, 1300, 39867, 17576]\n",
      "Ridge regression: train loss is  0.9108980021336966\n",
      "[11065, 3796, 119551, 53088]\n",
      "[3785, 1321, 39665, 17729]\n",
      "Ridge regression: train loss is  0.9103998827233454\n",
      "[11253, 3847, 119411, 52989]\n",
      "[3677, 1277, 39798, 17748]\n",
      "Ridge regression: train loss is  0.9109152840150062\n",
      "[11207, 3876, 119352, 53065]\n",
      "[3720, 1257, 39848, 17675]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91084805\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91088178\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "88.89999999999999\n",
      "Ridge regression: train loss is  0.9112178136475154\n",
      "[11271, 3889, 119277, 53063]\n",
      "[3746, 1295, 39872, 17587]\n",
      "Ridge regression: train loss is  0.910936779517913\n",
      "[11046, 3785, 119562, 53107]\n",
      "[3778, 1315, 39671, 17736]\n",
      "Ridge regression: train loss is  0.9104393558553315\n",
      "[11234, 3831, 119427, 53008]\n",
      "[3672, 1270, 39805, 17753]\n",
      "Ridge regression: train loss is  0.9109546103613033\n",
      "[11195, 3864, 119364, 53077]\n",
      "[3716, 1252, 39853, 17679]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91088714\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91092078\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "89.90909090909089\n",
      "Ridge regression: train loss is  0.911256602722229\n",
      "[11237, 3884, 119282, 53097]\n",
      "[3738, 1290, 39877, 17595]\n",
      "Ridge regression: train loss is  0.9109755444533335\n",
      "[11024, 3773, 119574, 53129]\n",
      "[3773, 1307, 39679, 17741]\n",
      "Ridge regression: train loss is  0.9104788117127787\n",
      "[11214, 3825, 119433, 53028]\n",
      "[3662, 1266, 39809, 17763]\n",
      "Ridge regression: train loss is  0.910993922953655\n",
      "[11173, 3854, 119374, 53099]\n",
      "[3702, 1250, 39855, 17693]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91092622\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91095977\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "90.91818181818181\n",
      "Ridge regression: train loss is  0.9112953790175299\n",
      "[11220, 3874, 119292, 53114]\n",
      "[3730, 1285, 39882, 17603]\n",
      "Ridge regression: train loss is  0.9110142953886533\n",
      "[11005, 3760, 119587, 53148]\n",
      "[3765, 1304, 39682, 17749]\n",
      "Ridge regression: train loss is  0.9105182488113187\n",
      "[11198, 3809, 119449, 53044]\n",
      "[3655, 1263, 39812, 17770]\n",
      "Ridge regression: train loss is  0.9110332202175783\n",
      "[11152, 3846, 119382, 53120]\n",
      "[3698, 1245, 39860, 17697]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91096529\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91099875\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "91.92727272727272\n",
      "Ridge regression: train loss is  0.9113341410016939\n",
      "[11196, 3866, 119300, 53138]\n",
      "[3720, 1284, 39883, 17613]\n",
      "Ridge regression: train loss is  0.9110530308118345\n",
      "[10991, 3749, 119598, 53162]\n",
      "[3757, 1302, 39684, 17757]\n",
      "Ridge regression: train loss is  0.9105576657047341\n",
      "[11177, 3791, 119467, 53065]\n",
      "[3644, 1258, 39817, 17781]\n",
      "Ridge regression: train loss is  0.9110725006192185\n",
      "[11129, 3825, 119403, 53143]\n",
      "[3689, 1239, 39866, 17706]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91100433\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91103771\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "92.93636363636362\n",
      "Ridge regression: train loss is  0.9113728871824263\n",
      "[11172, 3856, 119310, 53162]\n",
      "[3712, 1280, 39887, 17621]\n",
      "Ridge regression: train loss is  0.9110917492491305\n",
      "[10969, 3735, 119612, 53184]\n",
      "[3751, 1299, 39687, 17763]\n",
      "Ridge regression: train loss is  0.9105970609839987\n",
      "[11143, 3779, 119479, 53099]\n",
      "[3638, 1250, 39825, 17787]\n",
      "Ridge regression: train loss is  0.9111117626643044\n",
      "[11102, 3808, 119420, 53170]\n",
      "[3679, 1232, 39873, 17716]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91104337\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91107665\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "93.94545454545454\n",
      "Ridge regression: train loss is  0.91141161610584\n",
      "[11155, 3839, 119327, 53179]\n",
      "[3704, 1279, 39888, 17629]\n",
      "Ridge regression: train loss is  0.9111304492641358\n",
      "[10945, 3722, 119625, 53208]\n",
      "[3740, 1293, 39693, 17774]\n",
      "Ridge regression: train loss is  0.9106364332763478\n",
      "[11125, 3769, 119489, 53117]\n",
      "[3629, 1245, 39830, 17796]\n",
      "Ridge regression: train loss is  0.9111510048971359\n",
      "[11074, 3793, 119435, 53198]\n",
      "[3672, 1230, 39875, 17723]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91108238\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91111557\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "94.95454545454544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: train loss is  0.9114503263554664\n",
      "[11132, 3823, 119343, 53202]\n",
      "[3697, 1274, 39893, 17636]\n",
      "Ridge regression: train loss is  0.9111691294568669\n",
      "[10915, 3706, 119641, 53238]\n",
      "[3736, 1287, 39699, 17778]\n",
      "Ridge regression: train loss is  0.9106757812443744\n",
      "[11096, 3756, 119502, 53146]\n",
      "[3624, 1243, 39832, 17801]\n",
      "Ridge regression: train loss is  0.9111902258996009\n",
      "[11052, 3783, 119445, 53220]\n",
      "[3664, 1225, 39880, 17731]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91112137\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91115448\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "95.96363636363635\n",
      "Ridge regression: train loss is  0.9114890165512964\n",
      "[11108, 3803, 119363, 53226]\n",
      "[3685, 1269, 39898, 17648]\n",
      "Ridge regression: train loss is  0.9112077884628683\n",
      "[10899, 3692, 119655, 53254]\n",
      "[3731, 1280, 39706, 17783]\n",
      "Ridge regression: train loss is  0.9107151035851535\n",
      "[11070, 3744, 119514, 53172]\n",
      "[3620, 1240, 39835, 17805]\n",
      "Ridge regression: train loss is  0.9112294242902287\n",
      "[11026, 3769, 119459, 53246]\n",
      "[3659, 1217, 39888, 17736]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91116033\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91119336\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "96.97272727272727\n",
      "Ridge regression: train loss is  0.9115276853488534\n",
      "[11093, 3784, 119382, 53241]\n",
      "[3677, 1265, 39902, 17656]\n",
      "Ridge regression: train loss is  0.9112464249523462\n",
      "[10872, 3677, 119670, 53281]\n",
      "[3722, 1276, 39710, 17792]\n",
      "Ridge regression: train loss is  0.9107543990293935\n",
      "[11043, 3733, 119525, 53199]\n",
      "[3604, 1236, 39839, 17821]\n",
      "Ridge regression: train loss is  0.9112685987232662\n",
      "[11006, 3755, 119473, 53266]\n",
      "[3656, 1216, 39889, 17739]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91119928\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91123222\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "97.98181818181817\n",
      "Ridge regression: train loss is  0.9115663314382929\n",
      "[11071, 3773, 119393, 53263]\n",
      "[3671, 1262, 39905, 17662]\n",
      "Ridge regression: train loss is  0.9112850376293261\n",
      "[10848, 3664, 119683, 53305]\n",
      "[3714, 1272, 39714, 17800]\n",
      "Ridge regression: train loss is  0.9107936663406105\n",
      "[11017, 3723, 119535, 53225]\n",
      "[3598, 1228, 39847, 17827]\n",
      "Ridge regression: train loss is  0.9113077478877851\n",
      "[10979, 3748, 119480, 53293]\n",
      "[3653, 1210, 39895, 17742]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91123820\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91127105\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "98.99090909090908\n",
      "Ridge regression: train loss is  0.9116049535435331\n",
      "[11049, 3755, 119411, 53285]\n",
      "[3659, 1258, 39909, 17674]\n",
      "Ridge regression: train loss is  0.911323625230835\n",
      "[10826, 3650, 119697, 53327]\n",
      "[3709, 1268, 39718, 17805]\n",
      "Ridge regression: train loss is  0.9108329043143278\n",
      "[10997, 3709, 119549, 53245]\n",
      "[3594, 1222, 39853, 17831]\n",
      "Ridge regression: train loss is  0.9113468705068164\n",
      "[10953, 3737, 119491, 53319]\n",
      "[3642, 1208, 39897, 17753]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91127709\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91130986\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "100.0\n",
      "Ridge regression: train loss is  0.911643550421408\n",
      "[11027, 3744, 119422, 53307]\n",
      "[3652, 1249, 39918, 17681]\n",
      "Ridge regression: train loss is  0.9113621865261059\n",
      "[10810, 3640, 119707, 53343]\n",
      "[3702, 1265, 39721, 17812]\n",
      "Ridge regression: train loss is  0.9108721117772981\n",
      "[10973, 3697, 119561, 53269]\n",
      "[3584, 1219, 39856, 17841]\n",
      "Ridge regression: train loss is  0.9113859653365102\n",
      "[10932, 3723, 119505, 53340]\n",
      "[3632, 1202, 39903, 17763]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.91131595\n",
      "Averaged test rmse after 4-fold cross-validation = 0.91134865\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.06\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.28\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.28\n",
      "BEST LAMBDA = 0.1\n"
     ]
    }
   ],
   "source": [
    "max_acc, w_rr, _ , _, opt_lambda, _ = hyperparam(ridge_regression, lambda_=np.linspace(0.1, 100, 100));\n",
    "print(\"BEST LAMBDA =\", opt_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf12cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = r'C:/Users/Tomas/GitHub/ML_course/projects/project1/data/test1.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tx_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH =  r'C:/Users/Tomas/GitHub/ML_course/projects/project1/data/test_rr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_rr, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5b94a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
