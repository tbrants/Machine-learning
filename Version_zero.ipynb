{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a01387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import seaborn as sns\n",
    "from proj1_helpers import *\n",
    "\n",
    "SEED = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bfb138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "#its my datapath\n",
    "DATA_TRAIN_PATH = r'C:/Users/Tomas/GitHub/ML_course/projects/project1/data/train1.csv/train.csv' # TODO: download train data and supply path here\n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(y.shape)\n",
    "print(tx.shape)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9972ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse calculation\n",
    "def compute_error(y, tx, w):\n",
    "    e = y-np.matmul(tx, w)\n",
    "    return e\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = compute_error(y, tx, w)\n",
    "    mse = 1/(2*N)*np.sum(e**2)\n",
    "    rmse = np.sqrt(2*mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583fbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "def compute_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = compute_error(y, tx, w)\n",
    "    grad = (-1/N)*np.matmul(tx.T,e)\n",
    "    return grad\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w -= gamma*grad\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    print(\"Gradient Descent: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da57d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic gradient descent\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = compute_error(y, tx, w)\n",
    "    grad = (-1/N)*np.matmul(tx.T,e)\n",
    "    return grad\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma,  batch_size = 1):\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for ymini, txmini in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            grad = compute_gradient(ymini, txmini, w)\n",
    "            w -= gamma*grad\n",
    "    loss = compute_loss(ymini, txmini, w)\n",
    "    print(\"Stochastic Gradient Descent: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a20d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#least squares\n",
    "def least_squares(y, tx):\n",
    "    N = len(y)\n",
    "    w = np.linalg.solve(np.matmul(tx.T, tx), np.matmul(tx.T,y))\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    print(\"Least squares: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d570b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression\n",
    "def ridge_regression(y, phi, lambda_):\n",
    "    N=len(phi)\n",
    "    lambda_acc=2*N*lambda_\n",
    "    kwad = np.matmul(phi.T,phi)\n",
    "    w = np.linalg.solve(kwad+lambda_acc*np.eye(kwad.shape[0]),np.matmul(phi.T,y))\n",
    "    loss = compute_loss(y, phi, w)\n",
    "    print(\"Ridge regression: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfd20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions for logistic regression\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    return 1/(1 + np.exp(-t))\n",
    "\n",
    "def calculate_loss_lr(y, tx, w):\n",
    "    eps = 1e-6\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(np.matmul(tx, w))\n",
    "    a = np.matmul(y.T, np.log(pred+eps)) \n",
    "    b = np.matmul((1-y).T, np.log(1-pred+eps))\n",
    "    loss = a+b\n",
    "    return np.sum(- loss)\n",
    "\n",
    "def calculate_gradient_lr(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(np.matmul(tx, w)) \n",
    "    grad = np.matmul(tx.T,(pred - y))\n",
    "    return grad\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    loss = calculate_loss_lr(y, tx, w) + lambda_*np.sum(w.T.dot(w))\n",
    "    gradient = calculate_gradient_lr(y,tx,w)+2*lambda_*w\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression gradient descent\n",
    "def logistic_regression_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        grad = calculate_gradient_lr(y, tx, w)\n",
    "        w -= gamma*grad\n",
    "        #loss = calculate_loss_lr(y, tx, w)\n",
    "        #print(loss)\n",
    "    loss = calculate_loss_lr(y, tx, w)\n",
    "    print(\"Logistic regression: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a719c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularized logistic regression\n",
    "def logistic_regression_reg(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        grad = penalized_logistic_regression(y, tx, w, lambda_)[1]\n",
    "        w -= gamma*grad\n",
    "    loss = penalized_logistic_regression(y, tx, w, lambda_)[0]\n",
    "    print(\"Regularized logistic regression: train loss is \", loss)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b128e",
   "metadata": {},
   "source": [
    "# 2) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb40150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def build_poly_2d(x, degree):\\n    N,M = x.shape\\n    phi = np.zeros((N,M*(degree+1)))\\n    print(phi.shape)\\n    for i in range(M):\\n        col = x[:,i]\\n        exp = build_poly(col, degree)\\n        phi[:, i*(degree+1):(i+1)*(degree)] = exp\\n    return phi\\nx = np.array([[1,2,3],[4,5,6]])\\nprint(build_poly_2d(x, 1))'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#polynomial feature augmentation\n",
    "def build_poly(x, degree):\n",
    "    #prof flammarion said it is possible, but i would wait a bit with doing it\n",
    "    return x\n",
    "\"\"\"def build_poly(x, degree):\n",
    "    if degree == 0:\n",
    "        return x\n",
    "    else:\n",
    "        N = len(x)\n",
    "        phi = np.zeros((N,degree+1))\n",
    "        for i in range(degree+1):\n",
    "            phi[:, i] = x**i\n",
    "        return phi\"\"\"\n",
    "\n",
    "\"\"\"def build_poly_2d(x, degree):\n",
    "    N,M = x.shape\n",
    "    phi = np.zeros((N,M*(degree+1)))\n",
    "    print(phi.shape)\n",
    "    for i in range(M):\n",
    "        col = x[:,i]\n",
    "        exp = build_poly(col, degree)\n",
    "        phi[:, i*(degree+1):(i+1)*(degree)] = exp\n",
    "    return phi\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "print(build_poly_2d(x, 1))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "254e77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    TP = len(np.where((y_true == 1) & (y_pred == 1))[0])\n",
    "    FP = len(np.where((y_true == -1) & (y_pred == 1))[0])\n",
    "    TN = len(np.where((y_true == -1) & (y_pred == -1))[0])\n",
    "    FN = len(np.where((y_true == 1) & (y_pred == -1))[0])\n",
    "    print([TP,FP,TN,FN])\n",
    "    if TP+FN>0:\n",
    "        Recall = TP/(TP+FN)\n",
    "    else:\n",
    "        Recall = 0.0001\n",
    "    if TP+FP>0:\n",
    "        Precision = TP/(TP+FP)\n",
    "    else:\n",
    "        Precision = 0.00001\n",
    "    return 2*Recall*Precision/(Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7491bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    TP = len(np.where((y_true == 1) & (y_pred == 1))[0])\n",
    "    return (TP / y_true.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac9acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, opt_method, initial_w, max_iters, gamma, lambda_):\n",
    "    x_train_0, y_train_0 = x[k_indices[:k].ravel()], y[k_indices[:k].ravel()]\n",
    "    x_train_1, y_train_1 = x[k_indices[k+1:].ravel()], y[k_indices[k+1:].ravel()]\n",
    "    if x_train_0.shape[0] == 0:\n",
    "        x_train = x_train_1\n",
    "        y_train = y_train_1\n",
    "    if x_train_1.shape[0] == 0:\n",
    "        x_train = x_train_0\n",
    "        y_train = y_train_0\n",
    "    else:\n",
    "        x_train, y_train = np.concatenate((x_train_0, x_train_1), axis=0), np.concatenate((y_train_0, y_train_1), axis=0)\n",
    "    \n",
    "    x_test, y_test = x[k_indices[k]], y[k_indices[k]]\n",
    "    # get k'th subgroup in test, others in train\n",
    "\n",
    "    \n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    if opt_method == least_squares_GD:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, initial_w, max_iters, gamma)\n",
    "        loss_te = compute_loss(y_test, phi_test, w)\n",
    "        \n",
    "        #print('least_squares_GD')\n",
    "\n",
    "    elif opt_method == least_squares_SGD:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, initial_w, max_iters, gamma,  batch_size = 1)\n",
    "        loss_te = compute_loss(y_test, phi_test, w)\n",
    "        #print('least_squares_SGD')\n",
    "    \n",
    "    elif opt_method == least_squares:\n",
    "        w, loss_tr = opt_method(y_train, phi_train)\n",
    "        loss_te = compute_loss(y_test, phi_test, w)\n",
    "        #print('least_squares')\n",
    "\n",
    "    elif opt_method == ridge_regression:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, lambda_)\n",
    "        loss_te = compute_loss(y_test, phi_test, w)\n",
    "        #print('ridge_regression')\n",
    "\n",
    "    elif opt_method == logistic_regression_gd:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, initial_w, max_iters, gamma)\n",
    "        loss_te = calculate_loss_lr(y_test, phi_test, w)\n",
    "        #print('logistic_regression_gd')\n",
    "\n",
    "    elif opt_method == logistic_regression_reg:\n",
    "        w, loss_tr = opt_method(y_train, phi_train, initial_w, max_iters, gamma, lambda_)\n",
    "        loss_te = penalized_logistic_regression(y_test, phi_test, w, lambda_)[0]\n",
    "        #print('reg_logistic_regression')\n",
    "\n",
    "    else:\n",
    "        w, loss = (None , None)\n",
    "        test_loss = None\n",
    "        #print(\"Method not found\")\n",
    "    \n",
    "    \n",
    "    y_pred_tr = predict_labels(w, phi_train)\n",
    "    y_pred_te = predict_labels(w, phi_test)\n",
    "    acc_train = accuracy(y_train, y_pred_tr)\n",
    "    f1_train = f1(y_train, y_pred_tr)\n",
    "    acc_test = accuracy(y_test, y_pred_te)\n",
    "    f1_test = f1(y_test, y_pred_te)\n",
    "    \n",
    "    loss = [loss_tr, loss_te]\n",
    "    acc = [acc_train, acc_test]\n",
    "    f1_val = [f1_train, f1_test]\n",
    "    \n",
    "    return loss, acc, f1_val, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5915f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cross_validation(y, x, k_indices, k_fold, degree, opt_method, initial_w = 0, max_iters = 0, gamma = 0, lambda_ = 0):\n",
    "    loss_tr = []\n",
    "    loss_te = []\n",
    "    acc_tr = []\n",
    "    acc_te = []\n",
    "    f1_tr = []\n",
    "    f1_te = []\n",
    "    for k in range(k_fold):\n",
    "        loss, acc, f1_val, w = cross_validation(y, x, k_indices, k, degree, opt_method, initial_w, max_iters, gamma, lambda_)\n",
    "        loss_tr.append(loss[0])\n",
    "        loss_te.append(loss[1])\n",
    "        acc_tr.append(acc[0])\n",
    "        acc_te.append(acc[1])\n",
    "        f1_tr.append(f1_val[0])\n",
    "        f1_te.append(f1_val[1])\n",
    "    rmse_tr = np.mean(np.array(loss_tr))\n",
    "    rmse_te = np.mean(np.array(loss_te))\n",
    "    acc_tr_ = np.mean(np.array(acc_tr))\n",
    "    acc_te_ = np.mean(np.array(acc_te))\n",
    "    f1_tr_ = np.mean(np.array(f1_tr))\n",
    "    f1_te_ = np.mean(np.array(f1_te))\n",
    "    print(\"Averaged train rmse after %d-fold cross-validation = %.8f\"%(k_fold, rmse_tr))\n",
    "    print(\"Averaged test rmse after %d-fold cross-validation = %.8f\"%(k_fold, rmse_te))\n",
    "    print(\"Averaged train accuracy after %d-fold cross-validation = %.2f\"%(k_fold, acc_tr_))\n",
    "    print(\"Averaged test accuracy after %d-fold cross-validation = %.2f\"%(k_fold, acc_te_))\n",
    "    print(\"Averaged train F1-score after %d-fold cross-validation = %.2f\"%(k_fold, f1_tr_))\n",
    "    print(\"Averaged test F1-score after %d-fold cross-validation = %.2f\"%(k_fold, f1_te_))\n",
    "    return rmse_tr, rmse_te, acc_tr_, acc_te_, f1_tr_, f1_te_, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29568aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4974.979\n",
      "-999.0\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tx.shape[1]))\n",
    "max_iters = 100\n",
    "gamma = 1e-11\n",
    "lambda_ = 0.4\n",
    "print(tx.max())\n",
    "print(tx.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ac7848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: train loss is  0.9992990653651919\n",
      "[24246, 27636, 95530, 40088]\n",
      "[7984, 9206, 31961, 13349]\n",
      "Gradient Descent: train loss is  0.9986007484781765\n",
      "[24040, 27654, 95693, 40113]\n",
      "[8188, 9173, 31813, 13326]\n",
      "Gradient Descent: train loss is  0.9979078271396561\n",
      "[24177, 27594, 95664, 40065]\n",
      "[8051, 9233, 31842, 13374]\n",
      "Gradient Descent: train loss is  0.9972292595530088\n",
      "[24223, 27607, 95621, 40049]\n",
      "[8005, 9218, 31887, 13390]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.99825923\n",
      "Averaged test rmse after 4-fold cross-validation = 0.99826187\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.13\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.13\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.42\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.42\n",
      "Stochastic Gradient Descent: train loss is  0.9998657822900414\n",
      "[24244, 27622, 95544, 40090]\n",
      "[7983, 9201, 31966, 13350]\n",
      "Stochastic Gradient Descent: train loss is  0.9875537454703321\n",
      "[24031, 27592, 95755, 40122]\n",
      "[8186, 9157, 31829, 13328]\n",
      "Stochastic Gradient Descent: train loss is  1.013950560495003\n",
      "[24168, 27541, 95717, 40074]\n",
      "[8048, 9208, 31867, 13377]\n",
      "Stochastic Gradient Descent: train loss is  1.0005256165190153\n",
      "[24215, 27535, 95693, 40057]\n",
      "[8001, 9190, 31915, 13394]\n",
      "Averaged train rmse after 4-fold cross-validation = 1.00047393\n",
      "Averaged test rmse after 4-fold cross-validation = 0.99561313\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.13\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.13\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.42\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.42\n",
      "Least squares: train loss is  0.8250485040346167\n",
      "[31638, 15336, 107830, 32696]\n",
      "[10480, 5019, 36148, 10853]\n",
      "Least squares: train loss is  0.8240421357405285\n",
      "[31546, 15253, 108094, 32607]\n",
      "[10626, 5124, 35862, 10888]\n",
      "Least squares: train loss is  0.8236623215251707\n",
      "[31677, 15405, 107853, 32565]\n",
      "[10455, 5073, 36002, 10970]\n",
      "Least squares: train loss is  0.8241386572570878\n",
      "[31673, 15327, 107901, 32599]\n",
      "[10598, 5211, 35894, 10797]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.82422290\n",
      "Averaged test rmse after 4-fold cross-validation = 0.82436795\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.17\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.17\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.57\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.57\n",
      "Ridge regression: train loss is  0.8250485040346167\n",
      "[31638, 15336, 107830, 32696]\n",
      "[10480, 5019, 36148, 10853]\n",
      "Ridge regression: train loss is  0.8240421357405285\n",
      "[31546, 15253, 108094, 32607]\n",
      "[10626, 5124, 35862, 10888]\n",
      "Ridge regression: train loss is  0.8236623215251707\n",
      "[31677, 15405, 107853, 32565]\n",
      "[10455, 5073, 36002, 10970]\n",
      "Ridge regression: train loss is  0.8241386572570878\n",
      "[31673, 15327, 107901, 32599]\n",
      "[10598, 5211, 35894, 10797]\n",
      "Averaged train rmse after 4-fold cross-validation = 0.82422290\n",
      "Averaged test rmse after 4-fold cross-validation = 0.82436795\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.17\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.17\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.57\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.57\n",
      "Logistic regression: train loss is  -559294.481179826\n",
      "[17913, 16707, 106459, 46421]\n",
      "[5936, 5535, 35632, 15397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-53f4dcbe4310>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: train loss is  -494621.3205694757\n",
      "[17804, 16708, 106639, 46349]\n",
      "[6057, 5559, 35427, 15457]\n",
      "Logistic regression: train loss is  -464172.741735844\n",
      "[17952, 16657, 106601, 46290]\n",
      "[5922, 5627, 35448, 15503]\n",
      "Logistic regression: train loss is  -441283.30050221784\n",
      "[17922, 16729, 106499, 46350]\n",
      "[5949, 5545, 35560, 15446]\n",
      "Averaged train rmse after 4-fold cross-validation = -489842.96099684\n",
      "Averaged test rmse after 4-fold cross-validation = -162965.83292609\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.10\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.10\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.36\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.36\n",
      "Regularized logistic regression: train loss is  -426900.13657984836\n",
      "[17916, 16716, 106450, 46418]\n",
      "[5937, 5534, 35633, 15396]\n",
      "Regularized logistic regression: train loss is  -421784.09206788853\n",
      "[17806, 16706, 106641, 46347]\n",
      "[6061, 5561, 35425, 15453]\n",
      "Regularized logistic regression: train loss is  -417931.8394017455\n",
      "[17951, 16651, 106607, 46291]\n",
      "[5919, 5624, 35451, 15506]\n",
      "Regularized logistic regression: train loss is  -408321.9818871063\n",
      "[17921, 16727, 106501, 46351]\n",
      "[5949, 5546, 35559, 15446]\n",
      "Averaged train rmse after 4-fold cross-validation = -418734.51248415\n",
      "Averaged test rmse after 4-fold cross-validation = -139545.10538116\n",
      "Averaged train accuracy after 4-fold cross-validation = 0.10\n",
      "Averaged test accuracy after 4-fold cross-validation = 0.10\n",
      "Averaged train F1-score after 4-fold cross-validation = 0.36\n",
      "Averaged test F1-score after 4-fold cross-validation = 0.36\n"
     ]
    }
   ],
   "source": [
    "#apply cross validation\n",
    "k_fold = 4\n",
    "degree = 1\n",
    "k_indices = build_k_indices(y, k_fold, SEED)\n",
    "\n",
    "methods = ['LS_GD', 'LS_SGD', 'LS', 'RIDGE', 'LR_GD', 'LR_SGD']\n",
    "\n",
    "results = np.zeros(4*6)\n",
    "results = results.reshape(6,4)\n",
    "\n",
    "rmse_tr_ls_gd, rmse_te_ls_gd, results[0,0], results[0,1], results[0,2], results[0,3], w_gd = apply_cross_validation(y, tx, k_indices, k_fold, degree, least_squares_GD, initial_w, max_iters, gamma, lambda_)\n",
    "\n",
    "rmse_tr_ls_sgd, rmse_te_ls_sgd, results[1,0], results[1,1], results[1,2], results[1,3], w_sgd = apply_cross_validation(y, tx, k_indices, k_fold, degree, least_squares_SGD, initial_w, max_iters, gamma, lambda_)\n",
    "#print(rmse_tr_ls_sgd, rmse_te_ls_sgd)\n",
    "\n",
    "rmse_tr_ls, rmse_te_ls, results[2,0], results[2,1], results[2,2], results[2,3], w_ls = apply_cross_validation(y, tx, k_indices, k_fold, degree, least_squares)\n",
    "#print(rmse_tr_ls, rmse_te_ls)\n",
    "\n",
    "rmse_tr_ridge, rmse_te_ridge, results[3,0], results[3,1], results[3,2], results[3,3], w_rr = apply_cross_validation(y, tx, k_indices, k_fold, degree, ridge_regression, lambda_)\n",
    "#print(rmse_tr_ridge, rmse_te_ridge)\n",
    "\n",
    "rmse_tr_lr_gd, rmse_te_lr_gd, results[4,0], results[4,1], results[4,2], results[4,3], w_lr= apply_cross_validation(y, tx, k_indices, k_fold, degree, logistic_regression_gd, initial_w, max_iters, gamma)\n",
    "#print(rmse_tr_lr_gd, rmse_te_lr_gd)\n",
    "\n",
    "rmse_tr_lr_reg, rmse_te_lr_reg, results[5,0], results[5,1], results[5,2], results[5,3], w_lr_reg = apply_cross_validation(y, tx, k_indices, k_fold, degree, logistic_regression_reg, initial_w, max_iters, gamma, lambda_)\n",
    "#print(rmse_tr_lr_reg, rmse_te_lr_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec8c563f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,3,figsize=(25,14), sharey=True)\n",
    "for res in np.arange(results.shape[0]):\n",
    "        #print(res//3, res%3)\n",
    "        barWidth = 0.3\n",
    "        bars = ('train accuracy', 'test accuracy', 'train f1', 'test f1')\n",
    "\n",
    "        # Create bars\n",
    "        height = results[res]\n",
    "        axs[res//3,res%3].bar(bars, height, color=(0.2, 0.4, 0.6, 0.8))\n",
    "        # Create names on the x-axis\n",
    "        axs[res//3,res%3].set_title(methods[res])\n",
    "        \n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf12cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = r'C:/Users/Tomas/GitHub/ML_course/projects/project1/data/test1.csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tx_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a47588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH =  r'C:/Users/Tomas/GitHub/ML_course/projects/project1/data/test_rr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_rr, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5b94a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
